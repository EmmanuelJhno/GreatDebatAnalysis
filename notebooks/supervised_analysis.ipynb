{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "with open('../data/gdn_common_sorted.txt') as file : \n",
    "    for j, line in enumerate(file): \n",
    "        if j > 0 : \n",
    "            res.append(line.split(' ')[:701])\n",
    "res = np.asarray(res)\n",
    "embedding = pd.DataFrame(res[:,1:], index=res[:,0])\n",
    "embeded_words = {}\n",
    "\n",
    "for word in embedding.index : \n",
    "    embeded_words[word.split('_')[0]] = word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/data_F.csv', sep=';') # lecture data set \n",
    "department = np.copy(data['zip_code'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence): \n",
    "    '''\n",
    "    return the sentence passed in argmuents with no punctuation\n",
    "    '''\n",
    "    sentence = sentence.replace('\\'',' ').lower()\n",
    "    table = sentence.maketrans('', '', string.punctuation)\n",
    "    sentence = [w.translate(table) for w in sentence]\n",
    "    sentence = ''.join(sentence)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def embed_answer(sentence): \n",
    "    sentence_embedding = []\n",
    "    sentence = preprocess(sentence)\n",
    "    for i, word in enumerate(sentence.split(' ')):\n",
    "        try : \n",
    "            if not embeded_words[word].endswith('_i'):\n",
    "                sentence_embedding.append(embedding.loc[embeded_words[word]].tolist())\n",
    "        except KeyError: \n",
    "            continue\n",
    "    \n",
    "    return(np.asarray(sentence_embedding, dtype=np.float64))\n",
    "   \n",
    "def compute_variance(sentence_embedding, eval_type='var'):\n",
    "    result = 0\n",
    "    if eval_type == 'var':\n",
    "        for index in range(len(sentence_embedding[0])):\n",
    "            result += np.var(sentence_embedding[:,index])\n",
    "            \n",
    "    elif eval_type == 'dist':\n",
    "        n = len(sentence_embedding)\n",
    "        for word_1_idx in range(n-1):\n",
    "            for word_2_idx in range(word_1_idx+1,n):\n",
    "                result += np.linalg.norm(sentence_embedding[word_1_idx]-sentence_embedding[word_2_idx])\n",
    "        result = result*2/(n*(n-1))\n",
    "\n",
    "    return result\n",
    "\n",
    "def compute_moyenne(sentence_embedding):\n",
    "    result = []\n",
    "    #print(len(sentence_embedding[0]))\n",
    "    for index in range(len(sentence_embedding[0])): # taille 700\n",
    "        result.append(np.sum(sentence_embedding[:,index])/len(sentence_embedding))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2445/2445 [00:39<00:00, 62.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "      <th>score_0</th>\n",
       "      <th>score_1</th>\n",
       "      <th>score_2</th>\n",
       "      <th>score_3</th>\n",
       "      <th>score_4</th>\n",
       "      <th>score_5</th>\n",
       "      <th>score_6</th>\n",
       "      <th>score_7</th>\n",
       "      <th>score_8</th>\n",
       "      <th>...</th>\n",
       "      <th>score_690</th>\n",
       "      <th>score_691</th>\n",
       "      <th>score_692</th>\n",
       "      <th>score_693</th>\n",
       "      <th>score_694</th>\n",
       "      <th>score_695</th>\n",
       "      <th>score_696</th>\n",
       "      <th>score_697</th>\n",
       "      <th>score_698</th>\n",
       "      <th>score_699</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002184</td>\n",
       "      <td>-0.015336</td>\n",
       "      <td>-0.015614</td>\n",
       "      <td>0.004679</td>\n",
       "      <td>0.032181</td>\n",
       "      <td>0.061739</td>\n",
       "      <td>-0.052617</td>\n",
       "      <td>0.034983</td>\n",
       "      <td>-0.054036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029986</td>\n",
       "      <td>0.039820</td>\n",
       "      <td>-0.003342</td>\n",
       "      <td>0.030171</td>\n",
       "      <td>-0.050199</td>\n",
       "      <td>0.039690</td>\n",
       "      <td>-0.009338</td>\n",
       "      <td>-0.014119</td>\n",
       "      <td>0.052214</td>\n",
       "      <td>-0.037971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.063287</td>\n",
       "      <td>-0.016981</td>\n",
       "      <td>0.022253</td>\n",
       "      <td>-0.004149</td>\n",
       "      <td>0.041670</td>\n",
       "      <td>0.072220</td>\n",
       "      <td>-0.055472</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>-0.045204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.019873</td>\n",
       "      <td>-0.005545</td>\n",
       "      <td>-0.014305</td>\n",
       "      <td>-0.030772</td>\n",
       "      <td>0.017193</td>\n",
       "      <td>0.039085</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.008392</td>\n",
       "      <td>-0.034817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   annotation   score_0   score_1   score_2   score_3   score_4   score_5  \\\n",
       "0         1.0  0.002184 -0.015336 -0.015614  0.004679  0.032181  0.061739   \n",
       "1         1.0  0.063287 -0.016981  0.022253 -0.004149  0.041670  0.072220   \n",
       "\n",
       "    score_6   score_7   score_8    ...      score_690  score_691  score_692  \\\n",
       "0 -0.052617  0.034983 -0.054036    ...       0.029986   0.039820  -0.003342   \n",
       "1 -0.055472  0.000335 -0.045204    ...       0.010781   0.019873  -0.005545   \n",
       "\n",
       "   score_693  score_694  score_695  score_696  score_697  score_698  score_699  \n",
       "0   0.030171  -0.050199   0.039690  -0.009338  -0.014119   0.052214  -0.037971  \n",
       "1  -0.014305  -0.030772   0.017193   0.039085   0.000324   0.008392  -0.034817  \n",
       "\n",
       "[2 rows x 701 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "for index in tqdm.tqdm(range(len(data))):\n",
    "    answer = embed_answer(data.iloc[index]['solution'])\n",
    "    if len(answer)>1:\n",
    "        # score de taille 700\n",
    "        scores.append(compute_moyenne(answer))#,compute_variance(answer, eval_type='var'),compute_variance(answer, eval_type='dist')])\n",
    "    else:\n",
    "        scores.append([np.nan]*700)\n",
    "        \n",
    "# Total = 15309\n",
    "scores_df = pd.DataFrame(data=np.array(scores),index=data.index, columns = ['score_'+str(i) for i in range(700)]) \n",
    "argumentation_scores = pd.concat([data['annotation'],scores_df], axis=1) \n",
    "argumentation_scores.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2445, 701)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "Name: annotation, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argumentation_scores = argumentation_scores.dropna()\n",
    "print(argumentation_scores.shape)\n",
    "X = argumentation_scores.drop('annotation',axis=1)\n",
    "Y = argumentation_scores['annotation']\n",
    "X.head(2)\n",
    "Y.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_1 = y_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbalanced weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Scores on train---------------\n",
      "[[1223  206]\n",
      " [ 112  415]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.92      0.86      0.88      1429\n",
      "        1.0       0.67      0.79      0.72       527\n",
      "\n",
      "avg / total       0.85      0.84      0.84      1956\n",
      "\n",
      "1956 621.0\n",
      "0.7229965156794425\n",
      "---------------Scores on test---------------\n",
      "[[288  64]\n",
      " [ 85  52]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.77      0.82      0.79       352\n",
      "        1.0       0.45      0.38      0.41       137\n",
      "\n",
      "avg / total       0.68      0.70      0.69       489\n",
      "\n",
      "489 137.0\n",
      "0.4110671936758893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear', C=9)\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "print(15*'-' +'Scores on train' + 15*'-' )\n",
    "y_pred = svclassifier.predict(X_train)\n",
    "print(confusion_matrix(y_pred,y_train))\n",
    "print(classification_report(y_pred, y_train))\n",
    "print(len(y_train), np.sum(y_train))\n",
    "print(f1_score(y_pred,y_train))\n",
    "\n",
    "print(15*'-' +'Scores on test' + 15*'-' )\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(len(y_test), np.sum(y_test))\n",
    "print(f1_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Balanced weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Scores on train---------------\n",
      "[[1003   74]\n",
      " [ 332  547]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.75      0.93      0.83      1077\n",
      "        1.0       0.88      0.62      0.73       879\n",
      "\n",
      "avg / total       0.81      0.79      0.79      1956\n",
      "\n",
      "1956 621.0\n",
      "0.7293333333333333\n",
      "---------------Scores on test---------------\n",
      "[[222 130]\n",
      " [ 41  96]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.84      0.63      0.72       352\n",
      "        1.0       0.42      0.70      0.53       137\n",
      "\n",
      "avg / total       0.73      0.65      0.67       489\n",
      "\n",
      "489 137.0\n",
      "0.5289256198347108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear', C=5, class_weight='balanced')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "print(15*'-' +'Scores on train' + 15*'-' )\n",
    "y_pred = svclassifier.predict(X_train)\n",
    "print(confusion_matrix(y_pred,y_train))\n",
    "print(classification_report(y_pred, y_train))\n",
    "print(len(y_train), np.sum(y_train))\n",
    "print(f1_score(y_pred,y_train))\n",
    "\n",
    "print(15*'-' +'Scores on test' + 15*'-' )\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(len(y_test), np.sum(y_test))\n",
    "print(f1_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Balanced weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Scores on train---------------\n",
      "[[989  80]\n",
      " [346 541]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.74      0.93      0.82      1069\n",
      "        1.0       0.87      0.61      0.72       887\n",
      "\n",
      "avg / total       0.80      0.78      0.78      1956\n",
      "\n",
      "1956 621.0\n",
      "0.7175066312997348\n",
      "---------------Scores on test---------------\n",
      "[[218 134]\n",
      " [ 43  94]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.84      0.62      0.71       352\n",
      "        1.0       0.41      0.69      0.52       137\n",
      "\n",
      "avg / total       0.72      0.64      0.66       489\n",
      "\n",
      "489 137.0\n",
      "0.5150684931506849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear', C=9, class_weight= {0:freq_1, 1:1-freq_1})\n",
    "svclassifier.fit(X_train, y_train)\n",
    "\n",
    "print(15*'-' +'Scores on train' + 15*'-' )\n",
    "y_pred = svclassifier.predict(X_train)\n",
    "print(confusion_matrix(y_pred,y_train))\n",
    "print(classification_report(y_pred, y_train))\n",
    "print(len(y_train), np.sum(y_train))\n",
    "print(f1_score(y_pred,y_train))\n",
    "\n",
    "print(15*'-' +'Scores on test' + 15*'-' )\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(len(y_test), np.sum(y_test))\n",
    "print(f1_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6822d6aef0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=700)\n",
    "pca.fit_transform(X_train)\n",
    "plt.plot(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=313)\n",
    "pca.fit_transform(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Scores on train---------------\n",
      "[[866 100]\n",
      " [469 521]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.65      0.90      0.75       966\n",
      "        1.0       0.84      0.53      0.65       990\n",
      "\n",
      "avg / total       0.74      0.71      0.70      1956\n",
      "\n",
      "1956 621.0\n",
      "0.6468032278088144\n",
      "---------------Scores on test---------------\n",
      "[[217 135]\n",
      " [ 26 111]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.62      0.73       352\n",
      "        1.0       0.45      0.81      0.58       137\n",
      "\n",
      "avg / total       0.77      0.67      0.69       489\n",
      "\n",
      "489 137.0\n",
      "0.5796344647519583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svclassifier = SVC(kernel='linear', C=6.03e-01, class_weight= 'balanced')\n",
    "svclassifier.fit(X_train_pca, y_train)\n",
    "\n",
    "print(15*'-' +'Scores on train' + 15*'-' )\n",
    "y_pred = svclassifier.predict(X_train_pca)\n",
    "print(confusion_matrix(y_pred,y_train))\n",
    "print(classification_report(y_pred, y_train))\n",
    "print(len(y_train), np.sum(y_train))\n",
    "print(f1_score(y_pred,y_train))\n",
    "\n",
    "print(15*'-' +'Scores on test' + 15*'-' )\n",
    "y_pred = svclassifier.predict(X_test_pca)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(len(y_test), np.sum(y_test))\n",
    "print(f1_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "best f1 : 5.98e-01 |  for C = 8.71e-01 with 274 components:  12%|█▏        | 95/800 [20:08<2:38:11, 13.46s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-649f77f86e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msvclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlog_10_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'balanced'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0msvclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cv_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "n_split = 1\n",
    "n_val = 800\n",
    "cv_parameters = {'log_10_C': [-1, 2],\n",
    "                 'n_comp':[10, 400] }\n",
    "scores = []\n",
    "log_10_cs = []\n",
    "n_comps = []\n",
    "\n",
    "progress_bar = trange(n_val, desc='Bar desc', leave=True)\n",
    "for n in progress_bar:\n",
    "    log_10_c = np.random.random() * (cv_parameters['log_10_C'][1] - cv_parameters['log_10_C'][0]) + cv_parameters['log_10_C'][0]\n",
    "    n_comp = int(np.random.random() * (cv_parameters['n_comp'][1] - cv_parameters['n_comp'][0]) + cv_parameters['n_comp'][0])\n",
    "    n_comps.append(n_comp)\n",
    "    log_10_cs.append(log_10_c)\n",
    "    f1_mean = []\n",
    "    for split in range(n_split): \n",
    "        \n",
    "        X_cv_train, X_val, y_cv_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=split)\n",
    "        \n",
    "        pca = PCA(n_components=n_comp)\n",
    "        \n",
    "        pca.fit_transform(X_train)\n",
    "        X_train_pca = pca.transform(X_cv_train)\n",
    "        X_test_pca = pca.transform(X_val)\n",
    "        \n",
    "        svclassifier = SVC(kernel='linear', C=10**log_10_c, class_weight= 'balanced')\n",
    "        svclassifier.fit(X_train_pca, y_cv_train)\n",
    "        \n",
    "        y_pred = svclassifier.predict(X_test_pca)\n",
    "        f1_mean.append(f1_score(y_pred,y_val))\n",
    "    scores.append(np.mean(f1_mean))\n",
    "    i_max = scores.index(max(scores))\n",
    "    progress_bar.set_description('best f1 : {:.2e} |  for C = {:.2e} with {} components'.format(scores[i_max], 10**log_10_cs[i_max], n_comps[i_max]))\n",
    "    progress_bar.refresh()\n",
    "    \n",
    "        \n",
    "i_max = scores.index(max(scores))\n",
    "print('best f1 : {:.2e} |  for C = {:.2e} with {} components'.format(scores[i_max], 10**log_10_cs[i_max], n_comps[i_max]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  zip_code  \\\n",
      "0               1   57000.0   \n",
      "1               5   95330.0   \n",
      "2               6   84750.0   \n",
      "3              11   33820.0   \n",
      "4              15    8500.0   \n",
      "5              20   28600.0   \n",
      "6              21   11390.0   \n",
      "7              29   49000.0   \n",
      "8              30   29280.0   \n",
      "9              31   33130.0   \n",
      "10             33   63730.0   \n",
      "11             36   59000.0   \n",
      "12             38       0.0   \n",
      "13             40   34000.0   \n",
      "14             42   60153.0   \n",
      "15             46   92400.0   \n",
      "16             48   92500.0   \n",
      "17             50   17138.0   \n",
      "18             53    1500.0   \n",
      "19             58   13390.0   \n",
      "20             62   73000.0   \n",
      "21             63   41290.0   \n",
      "22             64   82000.0   \n",
      "23             68   78130.0   \n",
      "24             74   78150.0   \n",
      "25             76   45100.0   \n",
      "26             77   41250.0   \n",
      "27             80   83140.0   \n",
      "28             81   67000.0   \n",
      "29             83   31200.0   \n",
      "...           ...       ...   \n",
      "66527      153744   75014.0   \n",
      "66528      153745   94300.0   \n",
      "66529      153747   35000.0   \n",
      "66530      153750   13490.0   \n",
      "66531      153751      49.0   \n",
      "66532      153752   49110.0   \n",
      "66533      153754   63270.0   \n",
      "66534      153755   91190.0   \n",
      "66535      153757    1030.0   \n",
      "66536      153759   31000.0   \n",
      "66537      153760   92300.0   \n",
      "66538      153763   92160.0   \n",
      "66539      153768   33390.0   \n",
      "66540      153771   92240.0   \n",
      "66541      153773   35000.0   \n",
      "66542      153775   63670.0   \n",
      "66543      153779   20167.0   \n",
      "66544      153781   57000.0   \n",
      "66545      153783   74160.0   \n",
      "66546      153786      75.0   \n",
      "66547      153791   36200.0   \n",
      "66548      153793   33270.0   \n",
      "66549      153797   38000.0   \n",
      "66550      153798   14700.0   \n",
      "66551      153800   94200.0   \n",
      "66552      153801      42.0   \n",
      "66553      153802   34600.0   \n",
      "66554      153803   12330.0   \n",
      "66555      153804   59960.0   \n",
      "66556      153807   45000.0   \n",
      "\n",
      "                                                 problem  \\\n",
      "0      Les problèmes auxquels se trouve confronté l’e...   \n",
      "1                                  La pollution de l'air   \n",
      "2      La biodiversité et la disparition de certaines...   \n",
      "3                                  La pollution de l'air   \n",
      "4                                  Pollution air et mers   \n",
      "5                                  La pollution de l'air   \n",
      "6        Les dérèglements climatiques (crue, sécheresse)   \n",
      "7                                  La pollution de l'air   \n",
      "8      L'écologie punitive qui concerne les citoyens ...   \n",
      "9                                  La pollution de l'air   \n",
      "10       Les dérèglements climatiques (crue, sécheresse)   \n",
      "11                                                  tous   \n",
      "12     diminution de la biodiversité et déreglement c...   \n",
      "13                                 La pollution de l'air   \n",
      "14     La biodiversité et la disparition de certaines...   \n",
      "15     les déchets des usines et des particuliers en ...   \n",
      "16       Les dérèglements climatiques (crue, sécheresse)   \n",
      "17     La biodiversité et la disparition de certaines...   \n",
      "18     La biodiversité et la disparition de certaines...   \n",
      "19       Les dérèglements climatiques (crue, sécheresse)   \n",
      "20                   les quatre propositions sont liées!   \n",
      "21       Les dérèglements climatiques (crue, sécheresse)   \n",
      "22              Réduire l'émission gaz à effet de serre    \n",
      "23                                 La pollution de l'air   \n",
      "24                                 La pollution de l'air   \n",
      "25                                 La pollution de l'air   \n",
      "26       Les dérèglements climatiques (crue, sécheresse)   \n",
      "27       Les dérèglements climatiques (crue, sécheresse)   \n",
      "28     Le PIB : Les indicateurs utilisés pour gérer n...   \n",
      "29     La biodiversité et la disparition de certaines...   \n",
      "...                                                  ...   \n",
      "66527                              La pollution de l'air   \n",
      "66528  La biodiversité et la disparition de certaines...   \n",
      "66529                   Vers une mobilité zéro émission.   \n",
      "66530  La pollution de l'air provoque les déréglement...   \n",
      "66531  La biodiversité et la disparition de certaines...   \n",
      "66532  La biodiversité et la disparition de certaines...   \n",
      "66533  Tous les problèmes sont liés , ils sont interd...   \n",
      "66534    Les dérèglements climatiques (crue, sécheresse)   \n",
      "66535    Les dérèglements climatiques (crue, sécheresse)   \n",
      "66536  La biodiversité et la disparition de certaines...   \n",
      "66537  Tous les dérèglements et disparitions de faune...   \n",
      "66538  Les pollutions de l'air (micro-particules) par...   \n",
      "66539    Les dérèglements climatiques (crue, sécheresse)   \n",
      "66540       La pollution de l'air, de l'eau, de la terre   \n",
      "66541                   Vers une mobilité zéro émission.   \n",
      "66542                              La pollution de l'air   \n",
      "66543                             La gestion des déchets   \n",
      "66544  Les enjeux environnementaux sont indissociable...   \n",
      "66545  La biodiversité et la disparition de certaines...   \n",
      "66546                          Les 4 précédents ensemble   \n",
      "66547  Tout est important, il faut arrêter de courir ...   \n",
      "66548  La biodiversité et la disparition de certaines...   \n",
      "66549  En fait cette question n'est pas logique car l...   \n",
      "66550    Les dérèglements climatiques (crue, sécheresse)   \n",
      "66551  La biodiversité et la disparition de certaines...   \n",
      "66552  l'invasion des éoliennes gigantesques qu'on no...   \n",
      "66553  La biodiversité et la disparition de certaines...   \n",
      "66554                        l'ensemble car tout est lié   \n",
      "66555    Les dérèglements climatiques (crue, sécheresse)   \n",
      "66556  La biodiversité et la disparition de certaines...   \n",
      "\n",
      "                                                solution  \n",
      "0      Les problèmes auxquels se trouve confronté l’e...  \n",
      "1      En matière d'émission de C02 tout n'est pas di...  \n",
      "2      Arrêter les pesticides , notamment le glyphosa...  \n",
      "3      - Réduire de manière drastique les déplacement...  \n",
      "4      Interdire très rapidement les emballages plast...  \n",
      "5      Pour lutter contre la pollution liée aux voitu...  \n",
      "6      favoriser logistiquement et financièrement  la...  \n",
      "7      Imposer à des constructeurs automobiles d’accé...  \n",
      "8      Il faut que l'effort à la transition écologiqu...  \n",
      "9      Gratuité  des transports en commun  Taxer le v...  \n",
      "10     Sortir de la société de consommation, favorise...  \n",
      "11     Prendre en compte le réel pour que l'action hu...  \n",
      "12     Permettre la mise en place d'une agriculture r...  \n",
      "13     Plutôt que d'embêter les français avec la limi...  \n",
      "14     Écouter les écologistes et tenir compte de leu...  \n",
      "15     lutter contre le gâchis alimentaire dans les r...  \n",
      "16     Trouver une solution au niveau mondial, et non...  \n",
      "17     Supprimer les insecticides, retrouver une agri...  \n",
      "18     Réduire considérablement la TVA sur les produi...  \n",
      "19     - taxer les carburants plus fortement et utili...  \n",
      "20     Réduire nos déchets. Cesser d'importer des pro...  \n",
      "21     Une vrai gestion des forêts, ........remplacer...  \n",
      "22     Que Mr Macron prenne connaissance d'un projet ...  \n",
      "23     revoir la position par rapport au moteur Diese...  \n",
      "24     les aides pour améliorer la transition écologi...  \n",
      "25     Les transports en commun: Il est regrettable q...  \n",
      "26     Il faut une réponse mondiale, même si la Franc...  \n",
      "27     Apporter une vraie contribution pour l'acces d...  \n",
      "28     Abandonner le PIB et le taux de croissance com...  \n",
      "29     Deja il faut que l’etat, collectivités territo...  \n",
      "...                                                  ...  \n",
      "66527  Limiter les sources de pollution en investissa...  \n",
      "66528  Supprimer tous les pesticides chimiques et eng...  \n",
      "66529  Vers une mobilité zéro émission. Moratoire sur...  \n",
      "66530  L'état tout seul ne peut pas tout résoudre, ma...  \n",
      "66531  La France doit commencer par respecter le droi...  \n",
      "66532  moins de produits phytosanitaire, aider à la c...  \n",
      "66533  Changer notre facon de travailler, legiferer, ...  \n",
      "66534  Réduire la consommation d'énergie, en particul...  \n",
      "66535  Diminuer la quantité d'énergie utilisée global...  \n",
      "66536  Taxer le kérozéne de bateaux de plaisance, de ...  \n",
      "66537  Favoriser le remplacement du fossile par le so...  \n",
      "66538  Remettre en cause radicalement le système prod...  \n",
      "66539  Les énergies renouvelables peinent à s’imposer...  \n",
      "66540  Je pense que c'est un problème vaste qui a de ...  \n",
      "66541  Vers une mobilité zéro émission. Moratoire sur...  \n",
      "66542  - Mener une politique \"transports en commun\" p...  \n",
      "66543  Par paliers temporels assez courts, réduire de...  \n",
      "66544  Diminuer l'anthropisation des sols, sortir de ...  \n",
      "66545  - Interdictions des pesticides, revenir à des ...  \n",
      "66546  Passer à un système respectueux de la planète ...  \n",
      "66547  Consommer mieux, arrêter l'obsolescence progra...  \n",
      "66548  - Cesser rapidement l'utilisation des produits...  \n",
      "66549  Prendre les rapports qui présentent des soluti...  \n",
      "66550  limiter la consommation , cesser de dire que l...  \n",
      "66551  Arrêt complet de l'utilisation de pesticides e...  \n",
      "66552  étudier et mettre en place des solutions alter...  \n",
      "66553  « Imaginez. Vous vous réveillez et quelque cho...  \n",
      "66554  Proposer aux foyers modestes un prêt à taux zé...  \n",
      "66555  Chacun doit avoir la volonté et le courage d' ...  \n",
      "66556  Réduire les principales causes de la perte de ...  \n",
      "\n",
      "[66557 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data_unnanotated = pd.read_csv('../data/dataset_1.csv') # lecture data set \n",
    "print(data_unnanotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.35138773918152\n",
      "---------------Scores on train---------------\n",
      "[[807 112]\n",
      " [528 509]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.88      0.72       919\n",
      "        1.0       0.82      0.49      0.61      1037\n",
      "\n",
      "avg / total       0.72      0.67      0.66      1956\n",
      "\n",
      "1956 621.0\n",
      "0.6139927623642943\n",
      "---------------Scores on test---------------\n",
      "[[213 139]\n",
      " [ 21 116]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.91      0.61      0.73       352\n",
      "        1.0       0.45      0.85      0.59       137\n",
      "\n",
      "avg / total       0.78      0.67      0.69       489\n",
      "\n",
      "489 137.0\n",
      "0.5918367346938775\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAc1klEQVR4nO3dfZRcdZ3n8ff31q2Hfkh3Op0mIY8dIA6GZ4mIDzOrRGdBxbhHHIPuyMyyh9EjRx1n18GZs6zDzjm77HhgdJf1LCM4DO4IDjpDVFbGFWZU1JgOg0AggSYJJDGQzmOnH+vpu3/cW53qSocUpDvVufV5ndOnqu69VfW73PD5/e7v/ur+zN0REZHkChpdABERmVkKehGRhFPQi4gknIJeRCThFPQiIgkXNroAtebPn++9vb2NLoaIyGll06ZN+9y9Z6p1sy7oe3t76evra3QxREROK2b24vHWqetGRCThFPQiIgmnoBcRSTgFvYhIwinoRUQSTkEvIpJwCnoRkYRLTNDvOTzKbf+4le37hhtdFBGRWSUxQb/vSJ6vPNLPC3uHGl0UEZFZJTFBn01HuzJeLDe4JCIis0tigj4XpgAYK5QaXBIRkdklOUEft+jHigp6EZFqiQn6bNyiHy+o60ZEpFpygl4tehGRKSUn6MM46NWiFxGZJDFBb2Zkw4BxtehFRCapK+jN7Eoz22pm/WZ20xTrs2Z2f7x+g5n1xsvTZnaPmT1lZs+a2Remt/iT5dIp9dGLiNQ4YdCbWQq4A7gKWAVca2araja7Hjjo7ucAtwO3xss/DGTd/QLgUuAPKpXATFCLXkTkWPW06C8D+t19m7vngfuAtTXbrAXuiZ8/AKwxMwMcaDOzEGgB8sDgtJR8Crl0Sn30IiI16gn6xcDOqte74mVTbuPuReAw0E0U+sPAHuAl4EvufqD2C8zsBjPrM7O+gYGB17wTFdkw0A+mRERqzPTF2MuAErAIWAH8kZmdVbuRu9/p7qvdfXVPz5STmNcll07pFggiIjXqCfrdwNKq10viZVNuE3fTdAL7gY8CP3D3grvvBR4DVp9soY8nl1aLXkSkVj1BvxFYaWYrzCwDrAPW12yzHrgufn4N8Ii7O1F3zRUAZtYGXA5smY6CTyUbphT0IiI1Thj0cZ/7jcDDwLPAt9x9s5ndYmYfiDe7C+g2s37gc0BlCOYdQLuZbSaqML7u7k9O905U5NKBum5ERGqE9Wzk7g8BD9Usu7nq+RjRUMra9w1NtXymZNNq0YuI1ErML2OhMupGLXoRkWqJCnqNuhEROVaigj4bBoyr60ZEZJJEBb1a9CIix0pW0Icp8qUypbI3uigiIrNGooL+6ATh6r4REalIVNDn4slHdKtiEZGjkhX06WjeWE0nKCJyVKKCfmLeWLXoRUQmJCroc2HUolcfvYjIUYkKerXoRUSOlaigr7Todb8bEZGjEhX02XSl60YtehGRimQFfVjpulGLXkSkIlFBn1OLXkTkGAkLerXoRURqJSros5XhlQp6EZEJiQr63MS9btR1IyJSkaigz2p4pYjIMRIV9OmUEZh+MCUiUi1RQW9m8eQjatGLiFQkKuhBE4SLiNRKXNDn0in10YuIVElc0GfDQKNuRESqJC7o1aIXEZkscUGfTafUohcRqZK8oA8DtehFRKokLuhz6RRjatGLiExIXtCHge51IyJSJXFBrz56EZHJEhf0OfXRi4hMkrigz6Y1jl5EpFrigj4Xahy9iEi15AV9/IMpd290UUREZoXEBX02DCg7FMsKehERSGDQVyYIV/eNiEgkgUGv6QRFRKolLug1naCIyGTJC/q4Ra/JR0REInUFvZldaWZbzazfzG6aYn3WzO6P128ws96qdRea2c/NbLOZPWVmuekr/rEqLXpNJygiEjlh0JtZCrgDuApYBVxrZqtqNrseOOju5wC3A7fG7w2BbwCfcPfzgHcChWkr/RRyatGLiExST4v+MqDf3be5ex64D1hbs81a4J74+QPAGjMz4LeBJ939VwDuvt/dZ7SpXRl1oxubiYhE6gn6xcDOqte74mVTbuPuReAw0A28AXAze9jMHjezz0/1BWZ2g5n1mVnfwMDAa92HSbKhRt2IiFSb6YuxIfAO4GPx478xszW1G7n7ne6+2t1X9/T0nNQXahy9iMhk9QT9bmBp1esl8bIpt4n75TuB/USt/x+7+z53HwEeAt50soV+NRNdN2rRi4gA9QX9RmClma0wswywDlhfs8164Lr4+TXAIx7dbOZh4AIza40rgH8FPDM9RZ9apetGLXoRkUh4og3cvWhmNxKFdgq42903m9ktQJ+7rwfuAu41s37gAFFlgLsfNLPbiCoLBx5y9+/P0L4A6roREal1wqAHcPeHiLpdqpfdXPV8DPjwcd77DaIhlqeELsaKiEyWuF/GHm3RK+hFRCCBQZ8KjHTKGNMvY0VEgAQGPUS3QRhXi15EBEho0OfSgVr0IiKxRAZ9VvPGiohMSGbQpwONuhERiSUy6HNhSjc1ExGJJTLo1aIXETkqkUGfUx+9iMiEZAZ9OtAPpkREYokM+myY0lSCIiKxRAa9WvQiIkclMug1jl5E5KhEBn1Oo25ERCYkNOjVohcRqUhk0GfDqEUfTXIlItLckhn0mjdWRGRCIoN+YoJwjbwREUlm0B+dTlD99CIiiQx6TScoInJUIoO+0qLX5CMiIgkNevXRi4gcldCgV4teRKQikUGfDSt99Ap6EZFEBn2lRa+uGxGRxAZ93KJX142ISDKDviUO+uHxYoNLIiLSeIkM+gUdOQKD3YfGGl0UEZGGS2TQZ8KAhR05dh0YaXRRREQaLpFBD7BkXiu7Do42uhgiIg2X3KDvamHnQbXoRUQSG/RLu1p5eXBMNzYTkaaX3KCf14o77NEFWRFpcokN+iVdLQDqvhGRppfYoF86rxWAnQd0QVZEmltig35hR44wMHapRS8iTS6xQZ8KjEVzW9ipIZYi0uQSG/QAS+e1qEUvIk2vrqA3syvNbKuZ9ZvZTVOsz5rZ/fH6DWbWW7N+mZkNmdl/mJ5i12fJ3Fb10YtI0zth0JtZCrgDuApYBVxrZqtqNrseOOju5wC3A7fWrL8N+L8nX9zXZum8FvYNjTOa11h6EWle9bToLwP63X2bu+eB+4C1NdusBe6Jnz8ArDEzAzCzDwLbgc3TU+T6VUbe7D6k7hsRaV71BP1iYGfV613xsim3cfcicBjoNrN24I+BPzv5or52E2Pp1X0jIk1spi/GfhG43d2HXm0jM7vBzPrMrG9gYGDavnxpVzyWXhdkRaSJhXVssxtYWvV6Sbxsqm12mVkIdAL7gbcA15jZfwfmAmUzG3P3/1n9Zne/E7gTYPXq1f56dmQq89uzZMJAd7EUkaZWT9BvBFaa2QqiQF8HfLRmm/XAdcDPgWuAR9zdgd+sbGBmXwSGakN+JgWBRXex1H3pRaSJnTDo3b1oZjcCDwMp4G5332xmtwB97r4euAu418z6gQNElcGssLRL96UXkeZWT4sed38IeKhm2c1Vz8eAD5/gM774Osp30pZ0tfCrXYca8dUiIrNCon8ZC9EQy0MjBY6MFRpdFBGRhkh+0Mcjb9R9IyLNKvFBf3QsvS7IikhzSnzQL+9uxQy+++QeyuVpG7kpInLaSHzQz23N8Jk1K/nur37Nn3//WaJRnyIizaOuUTenu8+sWcmhkQJ3P7adua1pPr1mZaOLJCJyyjRF0JsZN79/FUfGitz2w+c4u6ed9114ZqOLJSJySiS+66YiCIxbP3QB3W0Zfvzc9N1PR0RktmuaoAcIUwFn9bSxff9wo4siInLKNFXQA/R2t7F9n4JeRJpH8wX9/DYGjowzNF5sdFFERE6Jpgv6FfPbANihVr2INImmC/re7jjo1U8vIk2i+YJ+fnTvG7XoRaRZNF3Qt2ZCFnbk2L5P974RkebQdEEPUat++75XncZWRCQxmjLoV8xvY8d+tehFpDk0ZdD3drdxYDjP4VFNRiIiydecQa8hliLSRJoy6CfG0muIpYg0gaYM+mXzoslIdCsEEWkGTRn0uXSKRZ0t6roRkabQlEEPlSGWCnoRSb6mDfoV86O7WGpqQRFJuqYN+t7uNgbHihwc0RBLEUm2pg36ysgbdd+ISNI1bdBrLL2INIumDfqlXa0EGmIpIk2gaYM+EwZcsLiThze/rAuyIpJoTRv0AL/71l6e3zvET/v3NbooIiIzpqmD/uqLzmR+e4avP7aj0UUREZkxTR302TDFR9+ynEe27FVfvYgkVlMHPcC/vXwZ6ZRxz892NLooIiIzoumD/ow5Oa6+cBF/17eTwTH9eEpEkqfpgx7g99++guF8iW9t3NnoooiITDsFPXDBkk4uXd7F3254SUMtRSRxFPSxay9bxrZ9w2zYfqDRRRERmVYK+tj7LjiTObmQ+375UqOLIiIyrRT0sZZMig9evJiHnn6ZQyP5RhdHRGTa1BX0ZnalmW01s34zu2mK9Vkzuz9ev8HMeuPl7zGzTWb2VPx4xfQWf3pde9ky8sUy33l8d6OLIiIybU4Y9GaWAu4ArgJWAdea2aqaza4HDrr7OcDtwK3x8n3A1e5+AXAdcO90FXwmrFrUwUVLOrlvoy7Kikhy1NOivwzod/dt7p4H7gPW1myzFrgnfv4AsMbMzN3/xd1/HS/fDLSYWXY6Cj5T1l22jOdeGeLxlw41uigiItOinqBfDFQPMN8VL5tyG3cvAoeB7pptPgQ87u7jtV9gZjeYWZ+Z9Q0MDNRb9hlx9UWLaMuk+B+PPM9YodTQsoiITIdTcjHWzM4j6s75g6nWu/ud7r7a3Vf39PSciiIdV3s25A/f8wb+aesAv/O/f87uQ6MNLY+IyMmqJ+h3A0urXi+Jl025jZmFQCewP369BPh74OPu/sLJFvhU+Pe/eRZ3/u6lbB8Y5v1f+Qk/022MReQ0Vk/QbwRWmtkKM8sA64D1NdusJ7rYCnAN8Ii7u5nNBb4P3OTuj01XoU+F3z5vIQ/e+Hbmt2e54d5N7Do40ugiiYi8LicM+rjP/UbgYeBZ4FvuvtnMbjGzD8Sb3QV0m1k/8DmgMgTzRuAc4GYzeyL+O2Pa92KGnNXTzt2/92bcnc8/8CTlskbiiMjpx2bbMMLVq1d7X19fo4sxyd9ueIk/+funuGXteXz8rb2NLo6IyDHMbJO7r55qnX4ZW4drL1vKb72hh//60BZ2aIISETnNKOjrYGbc+qELCFPGDff28ejWvfpBlYicNhT0dTqzs4WvrLuEI2NFfv/rG3nvV37KD595pdHFEhE5IQX9a/Cuc8/gn//ju/iLay5kvFjihnv7+MW2/Y0ulojIq1LQv0aZMODDq5fy3RvfwfJ5rXzu/ic4PKopCEVk9lLQv05t2ZC/XHcJrxwZ5+YHn250cUREjktBfxIuXjqXz65ZyYNP/JoHn9CtjUVkdlLQn6RPvvNsLl3exZ985yke3bK30cURETmGgv4khamAOz76Jnrnt/Hv7tnIHY/2a+iliMwqCvppsLAzxwOfeBtXX7iIv3h4Kzfcu4m+HQd0ywQRmRXCRhcgKVoyKb687mIuWNzJl/5xKz985hUWduR4/4VncuMV5zC3NdPoIopIk9K9bmbA0HiRHz37Ct97cg+PbtlLz5wst3/kYi4/q3YuFhGR6aF73Zxi7dmQtRcv5q8+vppvf/Jt5NIprv2rX3DrD7ZoIhMROeXUoj8FhseL3PLdZ7i/L5qRcXl3K287u5uPvWU55y/ubHDpRCQJXq1Fr6A/hba+fITH+vfxsxf284tt+xkaL/LuNy7gs+9eqcAXkZOioJ+FBscK/PVjO/jaT7YxOFbkQ29awn96/xt10VZEXhf10c9CHbk0n16zkp/edAWffOfZ/MMTu3nP7T/mB0+/3OiiiUjCKOgbrCOX5o+vPJcHP/V2etqzfOIbm/gv33tGP7oSkWmjoJ8lzl/cyYM3vp3fe1svd/10O19cv1lhLyLTQj+YmkXSqYD/fPUqMmHAnT/eRqHs/Pna8wkCa3TRROQ0pqCfZcyML1x1LqnA+Oo/vcDTuw/zkTcv5eqLFtGRSze6eCJyGtKom1nK3blv406+/th2nntliFw64I1ndtDVmmFuS5pzFrTzrt84g3MXzsFMLX6RZqfhlacxd+fJXYf59uO72DYwzMGRPAeH8/z68BgACzty/OvzFvA7b17KeYs0Fl+kWb1a0KvrZpYzMy5aOpeLls6dtPyVwTH+eesAj2zZyzc37uSen7/I+Ys7uOr8Mzm7p50V89tY3t1KLp1qUMlFZLZQiz4BDo3kefCJX3Pfxp08u2dwYnlgsGxeKysXzOGNZ3bwjnPmc8myuaRTGmwlkjTqumkig2MFduwbZvu+YV7YO0T/wBDPvTLEtoEhyg5tmRSre+dxVk8by+e1sry7jWXdrSztaiUTqgIQOV2p66aJdOTSXLhkLhcumdzVc3i0wM9f2M9Pnh9g04sH2bjjACP50sT6wGDR3BbesGBO/NfOvLYMc3JpOnIhi+a20JbVPxeR05H+z20SnS1prjx/IVeevxCILvLuG8rz0oFhXtw/wov7R9i+b5jnXjnCT54foFA69kzvjDlZervbWNiZo2dOlp45WVbMb+ONCztY0tWi8f4is5SCvkmZ2URYX7p83qR1hVKZlw6McGikwJGxAodHC+w6OMqOfcO8eGCEX+06xN7BcUYLR88I2rMhS7paWNCRY0FHlvntWbrbs8xvz7Bifhurzuwg1LUBkYZQ0Msx0qmAs3vaT7jdkbEC/XuH2PLyEbbsGWT3oVH2Hhlny8uD7B/KU6yaM7ctk+LS3nmsPKOdMGWEgdGRS3POGe2sPGOOzghEZpCCXl63Obk0lyzr4pJlXcesK5edwbEC+4byPLtnkF9uP8CG7fvp23GAUtkplX1SRRBY9HlzciHdbRnO7mnn7DPaOWt+G/PaMsxtzdDREpINU6RTRjZM6eKxSJ006kYa5vBodEbQv/cIOw+McmSswOBYkYEj47wwMMSe+Edhx9OWSTGvPcO8tiydLVEl0ZFLM7c1zbzWTPQYVxJdrWlaMilSgREGAa2ZlH5jIImiUTcyK3W2pLl0eReXLj/2jACirqEX949weLTAoZHoWkG+WKJQcsYKJQ6OFDgwPM7+4Xx8HWGEwXjb6rOF48mEAZ0tadri0M+mU3TkQhZ05FjYkaO7PTNxltGRS9PRUnlMMycbqqtJThsKepm15uTSr2uKRXfnyHiRg8N5Do4UODiS59BInvFCmWLZKZbKjBRKHB4tMDhaYHi8xFihxFixHJ9l7GPvkXFKr1JZmEUXoDtyadqyKVoz4cRZQjYMyKVTzMmFdLak423CY7ZrSacIU0ZgEJjRkkkxJxdVPLp/kUwnBb0kjll0obcjl2Z59+v7jFLZGRwtcGSsyOBYIfobjZ+PRl1M0WOB0XyJofEiI/kSR8aKjBdLjBai54OjBeo4uZgksOhsIwwCUoGRDQNaMlHFkA0DwlTV8nSK1kyKtmwY3fCuNTrjyIYBmVRAJoz+Ktc0omVGJpUiHUbdWJlUQC4TPaqCSSYFvcgUUoHR1Zahq+3k5vAtl53hfJHh8VL8WGQ0H1UEY4USpTKU3CmXnZF8iaHxqHLJF4+efeRLZUbyJUbypXh5mWLJORJfzxiJK5pDI/nXXKnU7nNLOkUuHZ2R5NIpwsAIU0YqCGjLpCbOULLpgMAMMwgDm6hMKiOqUkFAOmUTlU3lIno6DEgHAWHKSKeiiiYdVz5hcPQzKxVZpRwpdZOdFAW9yAwKAov7+Wd+LoFyOeqyGhwtkC+VyRfLjBejx3yxTL5UmrSsUpGMx8tG8kVG82XGinFXVqFEsRSNkCqUneHxIv17hzg8WqBQKlP26DuLZWe8WDqpSuZEMqmAbFwBpQMjCKIKJV111pIyIzAjCKIhwkcrmWCiSy1MBRNdZamq91c+K6qIjDAVTFRygR3tXqtUPNkwOqsKUxZ9b1yeyuem4ufpVPS5YVzpNeqMSUEvkhBBYHS2pOlsacwENcVSXHnEw2cLNZVNoVSeWFYZXhst84l1ZY+usZTKznixzFihcvYTPR8vHq18ilXfkS+VKcfvK5dhqFic9N3jxegzSmWn7NFfseR1XbSfTpXgT1dVJGF8hpMKjDXnnsGfvm/VtH+vgl5EpkWYCghPsxGr7k6h5ORLZYpVlU5UCUQVlzsTlUilwqitcCoVR6UbrvJYiD+nUPKJCqlS0R39HqdULlMoOws7W2ZkP+sKejO7EvgykAK+5u7/rWZ9Fvgb4FJgP/ARd98Rr/sCcD1QAj7t7g9PW+lFRE6CmUUXpxP+47sT7p2ZpYA7gKuAVcC1ZlZ7bnE9cNDdzwFuB26N37sKWAecB1wJ/K/480RE5BSppxq7DOh3923ungfuA9bWbLMWuCd+/gCwxqKrDmuB+9x93N23A/3x54mIyClST9AvBnZWvd4VL5tyG3cvAoeB7jrfi5ndYGZ9ZtY3MDBQf+lFROSEZkXHlLvf6e6r3X11T09Po4sjIpIo9QT9bmBp1esl8bIptzGzEOgkuihbz3tFRGQG1RP0G4GVZrbCzDJEF1fX12yzHrgufn4N8IhHt8VcD6wzs6yZrQBWAr+cnqKLiEg9Tji80t2LZnYj8DDR8Mq73X2zmd0C9Ln7euAu4F4z6wcOEFUGxNt9C3gGKAKfcvfSlF8kIiIzQvejFxFJgFe7H/2sC3ozGwBePImPmA/sm6binC6acZ+hOfdb+9w8Xut+L3f3KUezzLqgP1lm1ne8Wi2pmnGfoTn3W/vcPKZzv2fF8EoREZk5CnoRkYRLYtDf2egCNEAz7jM0535rn5vHtO134vroRURksiS26EVEpIqCXkQk4RIT9GZ2pZltNbN+M7up0eWZCWa21MweNbNnzGyzmX0mXj7PzH5oZs/Hj12NLutMMLOUmf2LmX0vfr3CzDbEx/z++BYdiWFmc83sATPbYmbPmtlbm+FYm9kfxv++nzazb5pZLonH2szuNrO9ZvZ01bIpj69FvhLv/5Nm9qbX8l2JCPo6J0dJgiLwR+6+Crgc+FS8nzcBP3L3lcCP4tdJ9Bng2arXtwK3xxPeHCSaACdJvgz8wN3PBS4i2vdEH2szWwx8Gljt7ucT3XZlHck81n9NNCFTteMd36uI7hW2ErgB+Opr+aJEBD31TY5y2nP3Pe7+ePz8CNH/+IuZPPHLPcAHG1PCmWNmS4D3AV+LXxtwBdFEN5Cw/TazTuC3iO4jhbvn3f0QTXCsie7B1RLfCbcV2EMCj7W7/5jo3mDVjnd81wJ/45FfAHPN7Mx6vyspQV/XBCdJYma9wCXABmCBu++JV70MLGhQsWbSXwKfB8rx627gUDzRDSTvmK8ABoCvx91VXzOzNhJ+rN19N/Al4CWigD8MbCLZx7ra8Y7vSWVcUoK+qZhZO/Bt4LPuPli9Lr49dKLGzJrZ+4G97r6p0WU5hULgTcBX3f0SYJiabpqEHusuotbrCmAR0Max3RtNYTqPb1KCvmkmODGzNFHI/x93/068+JXKaVz8uLdR5Zshbwc+YGY7iLrlriDqv54bn95D8o75LmCXu2+IXz9AFPxJP9bvBra7+4C7F4DvEB3/JB/rasc7vieVcUkJ+nomRzntxf3SdwHPuvttVauqJ365DnjwVJdtJrn7F9x9ibv3Eh3bR9z9Y8CjRBPdQML2291fBnaa2W/Ei9YQzeuQ6GNN1GVzuZm1xv/eK/ud2GNd43jHdz3w8Xj0zeXA4aounhNz90T8Ae8FngNeAP600eWZoX18B9Gp3JPAE/Hfe4n6q38EPA/8P2Beo8s6g/8N3gl8L35+FtGMZf3A3wHZRpdvmvf1YqAvPt7/AHQ1w7EG/gzYAjwN3Atkk3isgW8SXYcoEJ3BXX+84wsY0cjCF4CniEYl1f1dugWCiEjCJaXrRkREjkNBLyKScAp6EZGEU9CLiCScgl5EJOEU9CIiCaegFxFJuP8PyUqAruopKKIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "start = time.time()\n",
    "unnanotated_embeded_data = data_unnanotated.head(10000)['solution'].map(lambda x : np.mean(embed_answer(x).reshape(-1,700),axis=0))\n",
    "print(time.time()-start)\n",
    "x = pd.DataFrame(np.asarray(unnanotated_embeded_data.tolist()).astype(np.float64))\n",
    "x = x.dropna()\n",
    "\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit_transform(x)\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "\n",
    "svclassifier = SVC(kernel='linear', C=6.03e-01, class_weight= 'balanced')\n",
    "svclassifier.fit(X_train_pca, y_train)\n",
    "\n",
    "print(15*'-' +'Scores on train' + 15*'-' )\n",
    "y_pred = svclassifier.predict(X_train_pca)\n",
    "print(confusion_matrix(y_pred,y_train))\n",
    "print(classification_report(y_pred, y_train))\n",
    "print(len(y_train), np.sum(y_train))\n",
    "print(f1_score(y_pred,y_train))\n",
    "\n",
    "print(15*'-' +'Scores on test' + 15*'-' )\n",
    "y_pred = svclassifier.predict(X_test_pca)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(len(y_test), np.sum(y_test))\n",
    "print(f1_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['pred'] = svclassifier.predict(pca.transform(x.drop('Department',axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['Department'] = data_unnanotated['zip_code'] // 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>690</th>\n",
       "      <th>691</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "      <th>694</th>\n",
       "      <th>695</th>\n",
       "      <th>696</th>\n",
       "      <th>697</th>\n",
       "      <th>698</th>\n",
       "      <th>699</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007224</td>\n",
       "      <td>-0.018928</td>\n",
       "      <td>-0.015692</td>\n",
       "      <td>0.004884</td>\n",
       "      <td>0.034927</td>\n",
       "      <td>0.064126</td>\n",
       "      <td>-0.054061</td>\n",
       "      <td>0.037434</td>\n",
       "      <td>-0.061646</td>\n",
       "      <td>-0.003507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029842</td>\n",
       "      <td>0.034282</td>\n",
       "      <td>-0.007757</td>\n",
       "      <td>0.030578</td>\n",
       "      <td>-0.046678</td>\n",
       "      <td>0.035420</td>\n",
       "      <td>-0.004989</td>\n",
       "      <td>-0.010232</td>\n",
       "      <td>0.047805</td>\n",
       "      <td>-0.037694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.061707</td>\n",
       "      <td>-0.019779</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>-0.002620</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>0.067803</td>\n",
       "      <td>-0.054617</td>\n",
       "      <td>-0.002381</td>\n",
       "      <td>-0.041077</td>\n",
       "      <td>-0.057780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011981</td>\n",
       "      <td>0.018971</td>\n",
       "      <td>-0.004546</td>\n",
       "      <td>-0.016217</td>\n",
       "      <td>-0.026465</td>\n",
       "      <td>0.018937</td>\n",
       "      <td>0.034689</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.011223</td>\n",
       "      <td>-0.032094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062479</td>\n",
       "      <td>-0.016062</td>\n",
       "      <td>0.022250</td>\n",
       "      <td>-0.007306</td>\n",
       "      <td>0.061340</td>\n",
       "      <td>0.070155</td>\n",
       "      <td>-0.063448</td>\n",
       "      <td>0.031735</td>\n",
       "      <td>-0.137925</td>\n",
       "      <td>-0.039277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010198</td>\n",
       "      <td>0.011956</td>\n",
       "      <td>-0.021904</td>\n",
       "      <td>0.014571</td>\n",
       "      <td>-0.027251</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>0.028321</td>\n",
       "      <td>0.043685</td>\n",
       "      <td>-0.010480</td>\n",
       "      <td>-0.013711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136225</td>\n",
       "      <td>-0.093007</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>0.026869</td>\n",
       "      <td>0.057067</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>-0.123075</td>\n",
       "      <td>0.012990</td>\n",
       "      <td>-0.082796</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088216</td>\n",
       "      <td>-0.011502</td>\n",
       "      <td>-0.079337</td>\n",
       "      <td>0.027293</td>\n",
       "      <td>-0.071953</td>\n",
       "      <td>-0.053504</td>\n",
       "      <td>-0.034882</td>\n",
       "      <td>0.111229</td>\n",
       "      <td>0.024426</td>\n",
       "      <td>-0.039842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037710</td>\n",
       "      <td>-0.054166</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>-0.036107</td>\n",
       "      <td>-0.003270</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>-0.063551</td>\n",
       "      <td>-0.018885</td>\n",
       "      <td>-0.050624</td>\n",
       "      <td>-0.046092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010539</td>\n",
       "      <td>0.062242</td>\n",
       "      <td>-0.013465</td>\n",
       "      <td>0.043487</td>\n",
       "      <td>-0.030116</td>\n",
       "      <td>0.037946</td>\n",
       "      <td>-0.004833</td>\n",
       "      <td>-0.031555</td>\n",
       "      <td>-0.021488</td>\n",
       "      <td>-0.015373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.044008</td>\n",
       "      <td>-0.030442</td>\n",
       "      <td>0.040979</td>\n",
       "      <td>0.014855</td>\n",
       "      <td>0.043957</td>\n",
       "      <td>0.119844</td>\n",
       "      <td>-0.064233</td>\n",
       "      <td>0.007401</td>\n",
       "      <td>-0.016551</td>\n",
       "      <td>-0.029035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035812</td>\n",
       "      <td>0.033511</td>\n",
       "      <td>-0.007084</td>\n",
       "      <td>-0.038157</td>\n",
       "      <td>-0.044625</td>\n",
       "      <td>-0.009350</td>\n",
       "      <td>0.006107</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.036696</td>\n",
       "      <td>-0.051071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.053828</td>\n",
       "      <td>-0.050106</td>\n",
       "      <td>0.017752</td>\n",
       "      <td>-0.033939</td>\n",
       "      <td>0.029088</td>\n",
       "      <td>0.034120</td>\n",
       "      <td>-0.135843</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>-0.090924</td>\n",
       "      <td>-0.006585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109167</td>\n",
       "      <td>0.017601</td>\n",
       "      <td>-0.046829</td>\n",
       "      <td>0.026287</td>\n",
       "      <td>-0.031210</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.068244</td>\n",
       "      <td>0.058199</td>\n",
       "      <td>0.045689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.013942</td>\n",
       "      <td>-0.088146</td>\n",
       "      <td>0.031556</td>\n",
       "      <td>0.075670</td>\n",
       "      <td>0.086891</td>\n",
       "      <td>0.087568</td>\n",
       "      <td>-0.108111</td>\n",
       "      <td>-0.010792</td>\n",
       "      <td>-0.053309</td>\n",
       "      <td>-0.015862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036565</td>\n",
       "      <td>-0.030476</td>\n",
       "      <td>-0.027754</td>\n",
       "      <td>-0.033668</td>\n",
       "      <td>-0.089159</td>\n",
       "      <td>-0.007970</td>\n",
       "      <td>-0.080292</td>\n",
       "      <td>0.054372</td>\n",
       "      <td>0.054673</td>\n",
       "      <td>-0.017321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.059265</td>\n",
       "      <td>-0.120530</td>\n",
       "      <td>0.025512</td>\n",
       "      <td>-0.027247</td>\n",
       "      <td>0.052245</td>\n",
       "      <td>0.124286</td>\n",
       "      <td>-0.072793</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>-0.032137</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038950</td>\n",
       "      <td>0.069045</td>\n",
       "      <td>-0.016575</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>-0.032397</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>-0.024131</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>0.022271</td>\n",
       "      <td>-0.049432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.103657</td>\n",
       "      <td>-0.133469</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>-0.045838</td>\n",
       "      <td>0.061361</td>\n",
       "      <td>0.102410</td>\n",
       "      <td>-0.082837</td>\n",
       "      <td>-0.070167</td>\n",
       "      <td>-0.066852</td>\n",
       "      <td>0.062904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>-0.054573</td>\n",
       "      <td>-0.067236</td>\n",
       "      <td>-0.089781</td>\n",
       "      <td>0.017827</td>\n",
       "      <td>-0.053541</td>\n",
       "      <td>0.044077</td>\n",
       "      <td>0.141332</td>\n",
       "      <td>-0.035521</td>\n",
       "      <td>-0.005752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.064616</td>\n",
       "      <td>-0.049552</td>\n",
       "      <td>0.030499</td>\n",
       "      <td>0.009020</td>\n",
       "      <td>-0.015054</td>\n",
       "      <td>0.010199</td>\n",
       "      <td>-0.080700</td>\n",
       "      <td>-0.071566</td>\n",
       "      <td>-0.153524</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048192</td>\n",
       "      <td>0.074088</td>\n",
       "      <td>0.033511</td>\n",
       "      <td>-0.005411</td>\n",
       "      <td>0.017689</td>\n",
       "      <td>0.040210</td>\n",
       "      <td>-0.001459</td>\n",
       "      <td>0.063801</td>\n",
       "      <td>0.066629</td>\n",
       "      <td>-0.003736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.014543</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.025951</td>\n",
       "      <td>0.009672</td>\n",
       "      <td>0.056159</td>\n",
       "      <td>0.053206</td>\n",
       "      <td>-0.028314</td>\n",
       "      <td>0.037358</td>\n",
       "      <td>-0.031785</td>\n",
       "      <td>-0.019825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071896</td>\n",
       "      <td>0.039093</td>\n",
       "      <td>0.020866</td>\n",
       "      <td>0.038385</td>\n",
       "      <td>-0.044749</td>\n",
       "      <td>0.045367</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>-0.015119</td>\n",
       "      <td>0.065568</td>\n",
       "      <td>-0.020077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.045103</td>\n",
       "      <td>-0.015980</td>\n",
       "      <td>0.060757</td>\n",
       "      <td>0.007426</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>0.046839</td>\n",
       "      <td>-0.032106</td>\n",
       "      <td>-0.026187</td>\n",
       "      <td>-0.082375</td>\n",
       "      <td>-0.017893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096461</td>\n",
       "      <td>0.040524</td>\n",
       "      <td>0.031188</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>-0.048225</td>\n",
       "      <td>0.049586</td>\n",
       "      <td>0.019818</td>\n",
       "      <td>0.039278</td>\n",
       "      <td>0.004457</td>\n",
       "      <td>-0.050517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.072935</td>\n",
       "      <td>-0.054166</td>\n",
       "      <td>-0.005637</td>\n",
       "      <td>-0.018112</td>\n",
       "      <td>0.083225</td>\n",
       "      <td>0.105857</td>\n",
       "      <td>-0.107807</td>\n",
       "      <td>-0.043617</td>\n",
       "      <td>-0.056923</td>\n",
       "      <td>-0.006193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027632</td>\n",
       "      <td>0.030111</td>\n",
       "      <td>-0.016738</td>\n",
       "      <td>-0.015255</td>\n",
       "      <td>-0.044684</td>\n",
       "      <td>-0.031177</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>-0.002033</td>\n",
       "      <td>0.049643</td>\n",
       "      <td>-0.047786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.061477</td>\n",
       "      <td>-0.064722</td>\n",
       "      <td>-0.033255</td>\n",
       "      <td>-0.021804</td>\n",
       "      <td>-0.052680</td>\n",
       "      <td>0.106589</td>\n",
       "      <td>-0.105772</td>\n",
       "      <td>-0.043240</td>\n",
       "      <td>-0.017142</td>\n",
       "      <td>-0.080966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121972</td>\n",
       "      <td>0.031985</td>\n",
       "      <td>-0.049662</td>\n",
       "      <td>0.092810</td>\n",
       "      <td>-0.021684</td>\n",
       "      <td>0.037253</td>\n",
       "      <td>0.049969</td>\n",
       "      <td>-0.018936</td>\n",
       "      <td>0.015811</td>\n",
       "      <td>0.004661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.032075</td>\n",
       "      <td>-0.104988</td>\n",
       "      <td>0.034171</td>\n",
       "      <td>-0.061664</td>\n",
       "      <td>0.035771</td>\n",
       "      <td>0.037696</td>\n",
       "      <td>-0.148799</td>\n",
       "      <td>-0.024485</td>\n",
       "      <td>-0.164675</td>\n",
       "      <td>0.083258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.057275</td>\n",
       "      <td>0.015993</td>\n",
       "      <td>-0.013844</td>\n",
       "      <td>-0.011650</td>\n",
       "      <td>0.004270</td>\n",
       "      <td>-0.047628</td>\n",
       "      <td>0.075499</td>\n",
       "      <td>0.073852</td>\n",
       "      <td>-0.004789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.047072</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.047815</td>\n",
       "      <td>-0.006000</td>\n",
       "      <td>0.111863</td>\n",
       "      <td>0.126510</td>\n",
       "      <td>-0.042646</td>\n",
       "      <td>-0.004193</td>\n",
       "      <td>-0.010217</td>\n",
       "      <td>0.030054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016064</td>\n",
       "      <td>0.051290</td>\n",
       "      <td>-0.039258</td>\n",
       "      <td>0.006992</td>\n",
       "      <td>-0.074214</td>\n",
       "      <td>-0.005658</td>\n",
       "      <td>-0.027389</td>\n",
       "      <td>-0.030596</td>\n",
       "      <td>0.061851</td>\n",
       "      <td>-0.042742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.022100</td>\n",
       "      <td>-0.014815</td>\n",
       "      <td>0.033172</td>\n",
       "      <td>-0.011510</td>\n",
       "      <td>-0.020992</td>\n",
       "      <td>0.100287</td>\n",
       "      <td>-0.071238</td>\n",
       "      <td>0.067526</td>\n",
       "      <td>-0.058794</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.091787</td>\n",
       "      <td>0.017431</td>\n",
       "      <td>0.015878</td>\n",
       "      <td>-0.062211</td>\n",
       "      <td>0.050115</td>\n",
       "      <td>-0.005342</td>\n",
       "      <td>-0.015358</td>\n",
       "      <td>0.179090</td>\n",
       "      <td>-0.056513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.064157</td>\n",
       "      <td>-0.000507</td>\n",
       "      <td>0.023056</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.018745</td>\n",
       "      <td>0.095840</td>\n",
       "      <td>-0.034180</td>\n",
       "      <td>-0.009517</td>\n",
       "      <td>-0.074717</td>\n",
       "      <td>-0.012298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.026736</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>-0.046205</td>\n",
       "      <td>0.081218</td>\n",
       "      <td>0.020710</td>\n",
       "      <td>-0.024002</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.032666</td>\n",
       "      <td>-0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.045879</td>\n",
       "      <td>-0.005060</td>\n",
       "      <td>0.024461</td>\n",
       "      <td>-0.042516</td>\n",
       "      <td>0.025193</td>\n",
       "      <td>0.066981</td>\n",
       "      <td>-0.048814</td>\n",
       "      <td>-0.059648</td>\n",
       "      <td>-0.027014</td>\n",
       "      <td>-0.003172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077960</td>\n",
       "      <td>0.045765</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>-0.046760</td>\n",
       "      <td>-0.033504</td>\n",
       "      <td>0.015540</td>\n",
       "      <td>0.033273</td>\n",
       "      <td>0.037061</td>\n",
       "      <td>-0.005167</td>\n",
       "      <td>0.007517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.100951</td>\n",
       "      <td>-0.018380</td>\n",
       "      <td>0.070931</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.047216</td>\n",
       "      <td>0.110919</td>\n",
       "      <td>-0.034424</td>\n",
       "      <td>0.016152</td>\n",
       "      <td>-0.081509</td>\n",
       "      <td>-0.041772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061033</td>\n",
       "      <td>0.031002</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>0.028840</td>\n",
       "      <td>-0.039521</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>-0.007877</td>\n",
       "      <td>-0.048568</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>-0.025827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.113131</td>\n",
       "      <td>-0.047261</td>\n",
       "      <td>0.026314</td>\n",
       "      <td>-0.049981</td>\n",
       "      <td>0.056838</td>\n",
       "      <td>0.051953</td>\n",
       "      <td>-0.068589</td>\n",
       "      <td>0.022791</td>\n",
       "      <td>-0.041287</td>\n",
       "      <td>-0.007472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095265</td>\n",
       "      <td>0.025143</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>-0.018864</td>\n",
       "      <td>-0.065227</td>\n",
       "      <td>-0.015542</td>\n",
       "      <td>0.026593</td>\n",
       "      <td>0.029376</td>\n",
       "      <td>0.018339</td>\n",
       "      <td>-0.074801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.006424</td>\n",
       "      <td>0.012911</td>\n",
       "      <td>0.022585</td>\n",
       "      <td>0.037924</td>\n",
       "      <td>0.025049</td>\n",
       "      <td>0.151439</td>\n",
       "      <td>-0.058624</td>\n",
       "      <td>-0.055836</td>\n",
       "      <td>-0.015922</td>\n",
       "      <td>-0.041905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047669</td>\n",
       "      <td>0.018574</td>\n",
       "      <td>-0.027233</td>\n",
       "      <td>-0.005485</td>\n",
       "      <td>-0.040892</td>\n",
       "      <td>0.021075</td>\n",
       "      <td>0.050376</td>\n",
       "      <td>-0.033756</td>\n",
       "      <td>0.047922</td>\n",
       "      <td>-0.042465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.071635</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.034044</td>\n",
       "      <td>-0.008020</td>\n",
       "      <td>0.047661</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.061717</td>\n",
       "      <td>-0.008223</td>\n",
       "      <td>-0.047173</td>\n",
       "      <td>-0.016688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064130</td>\n",
       "      <td>0.043720</td>\n",
       "      <td>-0.014082</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>-0.054833</td>\n",
       "      <td>0.027680</td>\n",
       "      <td>-0.037300</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>0.007084</td>\n",
       "      <td>-0.065234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105973</td>\n",
       "      <td>-0.073456</td>\n",
       "      <td>-0.018815</td>\n",
       "      <td>-0.017458</td>\n",
       "      <td>0.047884</td>\n",
       "      <td>0.098860</td>\n",
       "      <td>-0.049849</td>\n",
       "      <td>0.043686</td>\n",
       "      <td>-0.138899</td>\n",
       "      <td>-0.091700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>0.028538</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>-0.038585</td>\n",
       "      <td>-0.067838</td>\n",
       "      <td>0.042451</td>\n",
       "      <td>-0.006711</td>\n",
       "      <td>-0.027048</td>\n",
       "      <td>0.041099</td>\n",
       "      <td>-0.006399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.050569</td>\n",
       "      <td>-0.038377</td>\n",
       "      <td>0.020018</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.066796</td>\n",
       "      <td>0.120994</td>\n",
       "      <td>-0.092250</td>\n",
       "      <td>0.029453</td>\n",
       "      <td>-0.068578</td>\n",
       "      <td>-0.069237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027413</td>\n",
       "      <td>0.011770</td>\n",
       "      <td>-0.027354</td>\n",
       "      <td>-0.013086</td>\n",
       "      <td>-0.036388</td>\n",
       "      <td>0.017089</td>\n",
       "      <td>0.017937</td>\n",
       "      <td>0.006725</td>\n",
       "      <td>0.076809</td>\n",
       "      <td>-0.026545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.005335</td>\n",
       "      <td>-0.041318</td>\n",
       "      <td>0.057945</td>\n",
       "      <td>0.009032</td>\n",
       "      <td>0.022111</td>\n",
       "      <td>0.092568</td>\n",
       "      <td>-0.086363</td>\n",
       "      <td>-0.017488</td>\n",
       "      <td>-0.022667</td>\n",
       "      <td>-0.031037</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028382</td>\n",
       "      <td>0.085795</td>\n",
       "      <td>-0.011994</td>\n",
       "      <td>0.026759</td>\n",
       "      <td>-0.066163</td>\n",
       "      <td>0.060391</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>-0.028126</td>\n",
       "      <td>-0.009446</td>\n",
       "      <td>-0.015308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.015425</td>\n",
       "      <td>0.019673</td>\n",
       "      <td>0.020462</td>\n",
       "      <td>-0.057373</td>\n",
       "      <td>0.085424</td>\n",
       "      <td>0.047833</td>\n",
       "      <td>-0.023647</td>\n",
       "      <td>-0.055611</td>\n",
       "      <td>-0.083917</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018890</td>\n",
       "      <td>0.025962</td>\n",
       "      <td>-0.008534</td>\n",
       "      <td>-0.072622</td>\n",
       "      <td>0.069724</td>\n",
       "      <td>-0.049076</td>\n",
       "      <td>0.006183</td>\n",
       "      <td>0.041978</td>\n",
       "      <td>-0.021082</td>\n",
       "      <td>-0.028592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.025480</td>\n",
       "      <td>-0.027217</td>\n",
       "      <td>0.052407</td>\n",
       "      <td>-0.049746</td>\n",
       "      <td>0.025650</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>-0.056980</td>\n",
       "      <td>0.006816</td>\n",
       "      <td>-0.051740</td>\n",
       "      <td>-0.001837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034779</td>\n",
       "      <td>-0.003818</td>\n",
       "      <td>0.054163</td>\n",
       "      <td>0.032721</td>\n",
       "      <td>-0.014674</td>\n",
       "      <td>0.037874</td>\n",
       "      <td>-0.007316</td>\n",
       "      <td>-0.079316</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>-0.079570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.147103</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.043236</td>\n",
       "      <td>0.070983</td>\n",
       "      <td>0.074668</td>\n",
       "      <td>0.078552</td>\n",
       "      <td>-0.026091</td>\n",
       "      <td>0.012504</td>\n",
       "      <td>-0.076657</td>\n",
       "      <td>0.051382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048832</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.050066</td>\n",
       "      <td>-0.009962</td>\n",
       "      <td>0.040372</td>\n",
       "      <td>0.128748</td>\n",
       "      <td>-0.022456</td>\n",
       "      <td>-0.011739</td>\n",
       "      <td>-0.041284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>0.063613</td>\n",
       "      <td>-0.022054</td>\n",
       "      <td>-0.039260</td>\n",
       "      <td>0.013037</td>\n",
       "      <td>0.105940</td>\n",
       "      <td>0.042220</td>\n",
       "      <td>-0.016665</td>\n",
       "      <td>0.027010</td>\n",
       "      <td>-0.105847</td>\n",
       "      <td>0.024720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016851</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>-0.000292</td>\n",
       "      <td>0.016578</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>0.014184</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>0.080414</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>-0.039569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>0.012482</td>\n",
       "      <td>-0.005987</td>\n",
       "      <td>0.027651</td>\n",
       "      <td>-0.016181</td>\n",
       "      <td>0.008615</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>-0.068497</td>\n",
       "      <td>-0.027949</td>\n",
       "      <td>-0.033807</td>\n",
       "      <td>0.100326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057753</td>\n",
       "      <td>0.086985</td>\n",
       "      <td>-0.000887</td>\n",
       "      <td>-0.002473</td>\n",
       "      <td>-0.007079</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>0.022413</td>\n",
       "      <td>0.056527</td>\n",
       "      <td>0.067364</td>\n",
       "      <td>-0.030107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>0.057686</td>\n",
       "      <td>-0.000828</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-0.022985</td>\n",
       "      <td>-0.052621</td>\n",
       "      <td>0.092183</td>\n",
       "      <td>-0.072151</td>\n",
       "      <td>-0.075296</td>\n",
       "      <td>-0.059033</td>\n",
       "      <td>-0.033685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069215</td>\n",
       "      <td>-0.053203</td>\n",
       "      <td>0.012437</td>\n",
       "      <td>-0.039112</td>\n",
       "      <td>-0.005080</td>\n",
       "      <td>-0.021198</td>\n",
       "      <td>0.007701</td>\n",
       "      <td>0.138480</td>\n",
       "      <td>-0.000740</td>\n",
       "      <td>-0.036735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>0.119178</td>\n",
       "      <td>-0.097167</td>\n",
       "      <td>-0.012568</td>\n",
       "      <td>-0.097261</td>\n",
       "      <td>0.026899</td>\n",
       "      <td>0.099830</td>\n",
       "      <td>-0.044245</td>\n",
       "      <td>-0.030462</td>\n",
       "      <td>-0.110869</td>\n",
       "      <td>0.011546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034062</td>\n",
       "      <td>-0.005004</td>\n",
       "      <td>-0.080140</td>\n",
       "      <td>-0.108227</td>\n",
       "      <td>0.040641</td>\n",
       "      <td>-0.029733</td>\n",
       "      <td>0.019564</td>\n",
       "      <td>0.070517</td>\n",
       "      <td>0.016961</td>\n",
       "      <td>-0.001265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>0.015446</td>\n",
       "      <td>-0.013531</td>\n",
       "      <td>-0.016089</td>\n",
       "      <td>0.013278</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>0.053577</td>\n",
       "      <td>-0.056140</td>\n",
       "      <td>0.035967</td>\n",
       "      <td>-0.026795</td>\n",
       "      <td>-0.002816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030413</td>\n",
       "      <td>0.050895</td>\n",
       "      <td>-0.003568</td>\n",
       "      <td>0.026351</td>\n",
       "      <td>-0.056430</td>\n",
       "      <td>0.035133</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>-0.023524</td>\n",
       "      <td>0.056665</td>\n",
       "      <td>-0.040537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>0.021344</td>\n",
       "      <td>-0.011522</td>\n",
       "      <td>0.067379</td>\n",
       "      <td>-0.046168</td>\n",
       "      <td>0.051516</td>\n",
       "      <td>0.073358</td>\n",
       "      <td>-0.080572</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>-0.036449</td>\n",
       "      <td>-0.035742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026605</td>\n",
       "      <td>0.026551</td>\n",
       "      <td>-0.019214</td>\n",
       "      <td>0.039910</td>\n",
       "      <td>-0.049488</td>\n",
       "      <td>0.031920</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>-0.011355</td>\n",
       "      <td>0.009898</td>\n",
       "      <td>-0.019989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>0.050626</td>\n",
       "      <td>-0.083236</td>\n",
       "      <td>-0.004474</td>\n",
       "      <td>-0.021741</td>\n",
       "      <td>0.057584</td>\n",
       "      <td>0.110085</td>\n",
       "      <td>-0.066982</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>-0.073528</td>\n",
       "      <td>0.046852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048373</td>\n",
       "      <td>0.022968</td>\n",
       "      <td>-0.038970</td>\n",
       "      <td>-0.044702</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>0.032874</td>\n",
       "      <td>0.020721</td>\n",
       "      <td>-0.001512</td>\n",
       "      <td>0.041173</td>\n",
       "      <td>0.003127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>0.037265</td>\n",
       "      <td>-0.179979</td>\n",
       "      <td>-0.045999</td>\n",
       "      <td>0.018884</td>\n",
       "      <td>-0.020336</td>\n",
       "      <td>0.117425</td>\n",
       "      <td>-0.113432</td>\n",
       "      <td>-0.007846</td>\n",
       "      <td>-0.083524</td>\n",
       "      <td>-0.026597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025975</td>\n",
       "      <td>-0.086432</td>\n",
       "      <td>-0.056930</td>\n",
       "      <td>-0.048261</td>\n",
       "      <td>-0.088495</td>\n",
       "      <td>-0.045814</td>\n",
       "      <td>0.049602</td>\n",
       "      <td>0.131018</td>\n",
       "      <td>-0.026412</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>0.035702</td>\n",
       "      <td>-0.044805</td>\n",
       "      <td>0.073181</td>\n",
       "      <td>0.009854</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.107482</td>\n",
       "      <td>-0.047310</td>\n",
       "      <td>-0.006987</td>\n",
       "      <td>-0.036043</td>\n",
       "      <td>-0.014955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032801</td>\n",
       "      <td>0.028598</td>\n",
       "      <td>0.055053</td>\n",
       "      <td>-0.098109</td>\n",
       "      <td>-0.017104</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.049645</td>\n",
       "      <td>0.007885</td>\n",
       "      <td>0.079296</td>\n",
       "      <td>0.030529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>0.038668</td>\n",
       "      <td>-0.021034</td>\n",
       "      <td>0.052645</td>\n",
       "      <td>0.012285</td>\n",
       "      <td>0.026097</td>\n",
       "      <td>0.076159</td>\n",
       "      <td>-0.039167</td>\n",
       "      <td>-0.037473</td>\n",
       "      <td>-0.060101</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027371</td>\n",
       "      <td>0.051686</td>\n",
       "      <td>0.012803</td>\n",
       "      <td>-0.058289</td>\n",
       "      <td>-0.011217</td>\n",
       "      <td>0.021801</td>\n",
       "      <td>-0.018431</td>\n",
       "      <td>0.047807</td>\n",
       "      <td>0.046113</td>\n",
       "      <td>-0.024265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>0.020183</td>\n",
       "      <td>-0.040838</td>\n",
       "      <td>0.033610</td>\n",
       "      <td>0.039336</td>\n",
       "      <td>0.067860</td>\n",
       "      <td>0.077764</td>\n",
       "      <td>-0.055728</td>\n",
       "      <td>0.007961</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.014514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041324</td>\n",
       "      <td>0.057160</td>\n",
       "      <td>0.010906</td>\n",
       "      <td>0.088287</td>\n",
       "      <td>-0.037321</td>\n",
       "      <td>0.057943</td>\n",
       "      <td>-0.007911</td>\n",
       "      <td>-0.024081</td>\n",
       "      <td>0.023898</td>\n",
       "      <td>-0.034964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>0.046118</td>\n",
       "      <td>-0.059672</td>\n",
       "      <td>0.065852</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>0.020464</td>\n",
       "      <td>0.098203</td>\n",
       "      <td>-0.063758</td>\n",
       "      <td>-0.027256</td>\n",
       "      <td>-0.079023</td>\n",
       "      <td>0.023003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>0.092852</td>\n",
       "      <td>0.020809</td>\n",
       "      <td>-0.007451</td>\n",
       "      <td>-0.009706</td>\n",
       "      <td>0.013813</td>\n",
       "      <td>-0.042984</td>\n",
       "      <td>0.010979</td>\n",
       "      <td>0.072838</td>\n",
       "      <td>-0.011917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>0.074308</td>\n",
       "      <td>-0.028507</td>\n",
       "      <td>-0.014141</td>\n",
       "      <td>-0.000380</td>\n",
       "      <td>0.052248</td>\n",
       "      <td>0.069941</td>\n",
       "      <td>-0.038499</td>\n",
       "      <td>0.009791</td>\n",
       "      <td>-0.082260</td>\n",
       "      <td>-0.017912</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005001</td>\n",
       "      <td>0.017491</td>\n",
       "      <td>0.021114</td>\n",
       "      <td>-0.026013</td>\n",
       "      <td>0.020584</td>\n",
       "      <td>0.035028</td>\n",
       "      <td>-0.004941</td>\n",
       "      <td>0.051360</td>\n",
       "      <td>0.038294</td>\n",
       "      <td>-0.017561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>0.048123</td>\n",
       "      <td>0.055133</td>\n",
       "      <td>0.090699</td>\n",
       "      <td>-0.002308</td>\n",
       "      <td>0.034291</td>\n",
       "      <td>0.038148</td>\n",
       "      <td>-0.010903</td>\n",
       "      <td>0.016494</td>\n",
       "      <td>-0.060275</td>\n",
       "      <td>-0.041975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013267</td>\n",
       "      <td>0.097718</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>-0.035394</td>\n",
       "      <td>-0.035664</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>-0.019229</td>\n",
       "      <td>-0.022298</td>\n",
       "      <td>0.052383</td>\n",
       "      <td>-0.030501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>0.078071</td>\n",
       "      <td>-0.087106</td>\n",
       "      <td>-0.000860</td>\n",
       "      <td>0.037389</td>\n",
       "      <td>0.055324</td>\n",
       "      <td>0.101819</td>\n",
       "      <td>-0.109899</td>\n",
       "      <td>0.014323</td>\n",
       "      <td>-0.055131</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035951</td>\n",
       "      <td>0.066272</td>\n",
       "      <td>-0.044217</td>\n",
       "      <td>0.021891</td>\n",
       "      <td>-0.089789</td>\n",
       "      <td>0.061103</td>\n",
       "      <td>0.015041</td>\n",
       "      <td>-0.024414</td>\n",
       "      <td>0.096560</td>\n",
       "      <td>-0.035556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>0.083080</td>\n",
       "      <td>-0.072640</td>\n",
       "      <td>-0.045678</td>\n",
       "      <td>-0.023598</td>\n",
       "      <td>0.038576</td>\n",
       "      <td>0.138184</td>\n",
       "      <td>-0.061783</td>\n",
       "      <td>-0.057908</td>\n",
       "      <td>-0.072600</td>\n",
       "      <td>0.029508</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001042</td>\n",
       "      <td>-0.014972</td>\n",
       "      <td>-0.026103</td>\n",
       "      <td>-0.038321</td>\n",
       "      <td>-0.056110</td>\n",
       "      <td>0.007828</td>\n",
       "      <td>0.029108</td>\n",
       "      <td>0.028697</td>\n",
       "      <td>0.086552</td>\n",
       "      <td>-0.004248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>-0.014342</td>\n",
       "      <td>-0.117543</td>\n",
       "      <td>-0.016599</td>\n",
       "      <td>-0.007097</td>\n",
       "      <td>-0.061239</td>\n",
       "      <td>0.126527</td>\n",
       "      <td>-0.086190</td>\n",
       "      <td>0.053103</td>\n",
       "      <td>-0.074879</td>\n",
       "      <td>0.017508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.055675</td>\n",
       "      <td>-0.037259</td>\n",
       "      <td>0.033994</td>\n",
       "      <td>-0.115524</td>\n",
       "      <td>-0.013510</td>\n",
       "      <td>0.070310</td>\n",
       "      <td>0.070588</td>\n",
       "      <td>0.037016</td>\n",
       "      <td>-0.067954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>0.107201</td>\n",
       "      <td>-0.061965</td>\n",
       "      <td>-0.012442</td>\n",
       "      <td>-0.037505</td>\n",
       "      <td>0.031311</td>\n",
       "      <td>0.030624</td>\n",
       "      <td>-0.047779</td>\n",
       "      <td>-0.028625</td>\n",
       "      <td>-0.059669</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058339</td>\n",
       "      <td>0.031198</td>\n",
       "      <td>0.021370</td>\n",
       "      <td>-0.078451</td>\n",
       "      <td>0.060494</td>\n",
       "      <td>0.037231</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>-0.011147</td>\n",
       "      <td>0.031787</td>\n",
       "      <td>-0.027944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>0.063790</td>\n",
       "      <td>-0.030417</td>\n",
       "      <td>0.006672</td>\n",
       "      <td>0.031840</td>\n",
       "      <td>0.017358</td>\n",
       "      <td>0.092839</td>\n",
       "      <td>-0.073205</td>\n",
       "      <td>0.034291</td>\n",
       "      <td>-0.022704</td>\n",
       "      <td>-0.026061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030154</td>\n",
       "      <td>0.022537</td>\n",
       "      <td>-0.000946</td>\n",
       "      <td>0.023961</td>\n",
       "      <td>-0.070943</td>\n",
       "      <td>0.022972</td>\n",
       "      <td>0.012036</td>\n",
       "      <td>-0.048032</td>\n",
       "      <td>0.096727</td>\n",
       "      <td>-0.078986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>0.129673</td>\n",
       "      <td>-0.035070</td>\n",
       "      <td>-0.019253</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>0.048483</td>\n",
       "      <td>0.066526</td>\n",
       "      <td>-0.103398</td>\n",
       "      <td>0.005715</td>\n",
       "      <td>-0.118712</td>\n",
       "      <td>0.022138</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011730</td>\n",
       "      <td>0.102465</td>\n",
       "      <td>-0.051305</td>\n",
       "      <td>-0.008031</td>\n",
       "      <td>-0.068241</td>\n",
       "      <td>0.154680</td>\n",
       "      <td>0.066076</td>\n",
       "      <td>-0.045540</td>\n",
       "      <td>0.084392</td>\n",
       "      <td>-0.029411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>0.075689</td>\n",
       "      <td>-0.069220</td>\n",
       "      <td>0.005614</td>\n",
       "      <td>-0.009541</td>\n",
       "      <td>0.057809</td>\n",
       "      <td>0.016492</td>\n",
       "      <td>-0.094435</td>\n",
       "      <td>-0.021130</td>\n",
       "      <td>-0.098352</td>\n",
       "      <td>-0.011267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090040</td>\n",
       "      <td>0.027887</td>\n",
       "      <td>-0.033888</td>\n",
       "      <td>-0.008269</td>\n",
       "      <td>-0.124192</td>\n",
       "      <td>0.036016</td>\n",
       "      <td>-0.062252</td>\n",
       "      <td>0.045104</td>\n",
       "      <td>0.046578</td>\n",
       "      <td>-0.020783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>0.060796</td>\n",
       "      <td>-0.036552</td>\n",
       "      <td>0.067404</td>\n",
       "      <td>0.028957</td>\n",
       "      <td>0.029521</td>\n",
       "      <td>0.032277</td>\n",
       "      <td>-0.033691</td>\n",
       "      <td>-0.012002</td>\n",
       "      <td>-0.051258</td>\n",
       "      <td>-0.006559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047390</td>\n",
       "      <td>0.025762</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>-0.004225</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.067068</td>\n",
       "      <td>-0.021656</td>\n",
       "      <td>0.012830</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>-0.027166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>0.031545</td>\n",
       "      <td>-0.006376</td>\n",
       "      <td>0.035787</td>\n",
       "      <td>0.008093</td>\n",
       "      <td>0.055254</td>\n",
       "      <td>0.090715</td>\n",
       "      <td>-0.078966</td>\n",
       "      <td>-0.005027</td>\n",
       "      <td>-0.056674</td>\n",
       "      <td>-0.038876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017271</td>\n",
       "      <td>0.047317</td>\n",
       "      <td>0.040798</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>-0.075576</td>\n",
       "      <td>0.010327</td>\n",
       "      <td>0.010834</td>\n",
       "      <td>-0.016392</td>\n",
       "      <td>0.074396</td>\n",
       "      <td>-0.036193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>0.034930</td>\n",
       "      <td>-0.063043</td>\n",
       "      <td>-0.034943</td>\n",
       "      <td>-0.015523</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.115061</td>\n",
       "      <td>-0.098961</td>\n",
       "      <td>0.008999</td>\n",
       "      <td>-0.022952</td>\n",
       "      <td>-0.035981</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102981</td>\n",
       "      <td>-0.030225</td>\n",
       "      <td>-0.014419</td>\n",
       "      <td>0.032484</td>\n",
       "      <td>-0.064793</td>\n",
       "      <td>-0.037512</td>\n",
       "      <td>0.071747</td>\n",
       "      <td>0.048783</td>\n",
       "      <td>0.063552</td>\n",
       "      <td>-0.025651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>0.034875</td>\n",
       "      <td>-0.057653</td>\n",
       "      <td>0.022750</td>\n",
       "      <td>-0.005472</td>\n",
       "      <td>0.022836</td>\n",
       "      <td>0.094878</td>\n",
       "      <td>-0.071061</td>\n",
       "      <td>-0.008741</td>\n",
       "      <td>-0.046399</td>\n",
       "      <td>-0.008402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024601</td>\n",
       "      <td>0.050609</td>\n",
       "      <td>0.010503</td>\n",
       "      <td>0.005921</td>\n",
       "      <td>-0.061036</td>\n",
       "      <td>0.023769</td>\n",
       "      <td>-0.023205</td>\n",
       "      <td>0.013706</td>\n",
       "      <td>0.061262</td>\n",
       "      <td>-0.043021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.023687</td>\n",
       "      <td>-0.013359</td>\n",
       "      <td>0.030009</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>0.019375</td>\n",
       "      <td>0.070721</td>\n",
       "      <td>-0.090010</td>\n",
       "      <td>-0.003628</td>\n",
       "      <td>-0.050935</td>\n",
       "      <td>-0.004465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031084</td>\n",
       "      <td>0.043216</td>\n",
       "      <td>-0.004038</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>-0.047682</td>\n",
       "      <td>0.052034</td>\n",
       "      <td>-0.000792</td>\n",
       "      <td>-0.022946</td>\n",
       "      <td>0.075094</td>\n",
       "      <td>-0.022445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.077506</td>\n",
       "      <td>-0.134495</td>\n",
       "      <td>-0.028633</td>\n",
       "      <td>-0.018219</td>\n",
       "      <td>-0.072727</td>\n",
       "      <td>0.098192</td>\n",
       "      <td>-0.187106</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>-0.056606</td>\n",
       "      <td>0.091683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100479</td>\n",
       "      <td>-0.015557</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.010442</td>\n",
       "      <td>-0.012845</td>\n",
       "      <td>0.009642</td>\n",
       "      <td>-0.078654</td>\n",
       "      <td>0.068639</td>\n",
       "      <td>0.158897</td>\n",
       "      <td>-0.055804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.073166</td>\n",
       "      <td>-0.029452</td>\n",
       "      <td>-0.003350</td>\n",
       "      <td>-0.016969</td>\n",
       "      <td>0.058090</td>\n",
       "      <td>0.107816</td>\n",
       "      <td>-0.062161</td>\n",
       "      <td>0.016016</td>\n",
       "      <td>-0.013372</td>\n",
       "      <td>-0.018690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014468</td>\n",
       "      <td>0.015748</td>\n",
       "      <td>-0.007151</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.001319</td>\n",
       "      <td>0.016347</td>\n",
       "      <td>0.029298</td>\n",
       "      <td>0.022122</td>\n",
       "      <td>-0.013759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.110653</td>\n",
       "      <td>-0.019663</td>\n",
       "      <td>0.055734</td>\n",
       "      <td>0.038825</td>\n",
       "      <td>0.017562</td>\n",
       "      <td>0.063298</td>\n",
       "      <td>-0.047049</td>\n",
       "      <td>0.002579</td>\n",
       "      <td>-0.088703</td>\n",
       "      <td>-0.008809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011645</td>\n",
       "      <td>0.025317</td>\n",
       "      <td>-0.028721</td>\n",
       "      <td>-0.052768</td>\n",
       "      <td>0.020667</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.037796</td>\n",
       "      <td>0.009136</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>-0.042717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.053051</td>\n",
       "      <td>-0.096866</td>\n",
       "      <td>0.059795</td>\n",
       "      <td>-0.005018</td>\n",
       "      <td>0.046272</td>\n",
       "      <td>0.098112</td>\n",
       "      <td>-0.031299</td>\n",
       "      <td>-0.015067</td>\n",
       "      <td>-0.084755</td>\n",
       "      <td>0.031297</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028131</td>\n",
       "      <td>0.062933</td>\n",
       "      <td>-0.016335</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>-0.061711</td>\n",
       "      <td>0.034415</td>\n",
       "      <td>0.037902</td>\n",
       "      <td>0.031869</td>\n",
       "      <td>-0.023133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9999 rows × 700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.007224 -0.018928 -0.015692  0.004884  0.034927  0.064126 -0.054061   \n",
       "1     0.061707 -0.019779  0.023700 -0.002620  0.036888  0.067803 -0.054617   \n",
       "2     0.062479 -0.016062  0.022250 -0.007306  0.061340  0.070155 -0.063448   \n",
       "3     0.136225 -0.093007  0.003619  0.026869  0.057067  0.102000 -0.123075   \n",
       "4     0.037710 -0.054166  0.011200 -0.036107 -0.003270  0.021288 -0.063551   \n",
       "5     0.044008 -0.030442  0.040979  0.014855  0.043957  0.119844 -0.064233   \n",
       "6     0.053828 -0.050106  0.017752 -0.033939  0.029088  0.034120 -0.135843   \n",
       "7     0.013942 -0.088146  0.031556  0.075670  0.086891  0.087568 -0.108111   \n",
       "8     0.059265 -0.120530  0.025512 -0.027247  0.052245  0.124286 -0.072793   \n",
       "9     0.103657 -0.133469  0.027737 -0.045838  0.061361  0.102410 -0.082837   \n",
       "10    0.064616 -0.049552  0.030499  0.009020 -0.015054  0.010199 -0.080700   \n",
       "11    0.014543 -0.001465  0.025951  0.009672  0.056159  0.053206 -0.028314   \n",
       "12    0.045103 -0.015980  0.060757  0.007426 -0.000600  0.046839 -0.032106   \n",
       "13    0.072935 -0.054166 -0.005637 -0.018112  0.083225  0.105857 -0.107807   \n",
       "14    0.061477 -0.064722 -0.033255 -0.021804 -0.052680  0.106589 -0.105772   \n",
       "15    0.032075 -0.104988  0.034171 -0.061664  0.035771  0.037696 -0.148799   \n",
       "16    0.047072 -0.000130  0.047815 -0.006000  0.111863  0.126510 -0.042646   \n",
       "17    0.022100 -0.014815  0.033172 -0.011510 -0.020992  0.100287 -0.071238   \n",
       "18    0.064157 -0.000507  0.023056  0.001275  0.018745  0.095840 -0.034180   \n",
       "19    0.045879 -0.005060  0.024461 -0.042516  0.025193  0.066981 -0.048814   \n",
       "20    0.100951 -0.018380  0.070931  0.002050  0.047216  0.110919 -0.034424   \n",
       "21    0.113131 -0.047261  0.026314 -0.049981  0.056838  0.051953 -0.068589   \n",
       "22    0.006424  0.012911  0.022585  0.037924  0.025049  0.151439 -0.058624   \n",
       "23    0.071635  0.000450  0.034044 -0.008020  0.047661  0.072588 -0.061717   \n",
       "24    0.105973 -0.073456 -0.018815 -0.017458  0.047884  0.098860 -0.049849   \n",
       "25    0.050569 -0.038377  0.020018  0.002208  0.066796  0.120994 -0.092250   \n",
       "26    0.005335 -0.041318  0.057945  0.009032  0.022111  0.092568 -0.086363   \n",
       "27   -0.015425  0.019673  0.020462 -0.057373  0.085424  0.047833 -0.023647   \n",
       "28    0.025480 -0.027217  0.052407 -0.049746  0.025650  0.095310 -0.056980   \n",
       "29    0.147103  0.001798  0.043236  0.070983  0.074668  0.078552 -0.026091   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9970  0.063613 -0.022054 -0.039260  0.013037  0.105940  0.042220 -0.016665   \n",
       "9971  0.012482 -0.005987  0.027651 -0.016181  0.008615  0.004687 -0.068497   \n",
       "9972  0.057686 -0.000828  0.000324 -0.022985 -0.052621  0.092183 -0.072151   \n",
       "9973  0.119178 -0.097167 -0.012568 -0.097261  0.026899  0.099830 -0.044245   \n",
       "9974  0.015446 -0.013531 -0.016089  0.013278  0.008482  0.053577 -0.056140   \n",
       "9975  0.021344 -0.011522  0.067379 -0.046168  0.051516  0.073358 -0.080572   \n",
       "9976  0.050626 -0.083236 -0.004474 -0.021741  0.057584  0.110085 -0.066982   \n",
       "9977  0.037265 -0.179979 -0.045999  0.018884 -0.020336  0.117425 -0.113432   \n",
       "9978  0.035702 -0.044805  0.073181  0.009854  0.001193  0.107482 -0.047310   \n",
       "9979  0.038668 -0.021034  0.052645  0.012285  0.026097  0.076159 -0.039167   \n",
       "9980  0.020183 -0.040838  0.033610  0.039336  0.067860  0.077764 -0.055728   \n",
       "9981  0.046118 -0.059672  0.065852  0.012682  0.020464  0.098203 -0.063758   \n",
       "9982  0.074308 -0.028507 -0.014141 -0.000380  0.052248  0.069941 -0.038499   \n",
       "9983  0.048123  0.055133  0.090699 -0.002308  0.034291  0.038148 -0.010903   \n",
       "9984  0.078071 -0.087106 -0.000860  0.037389  0.055324  0.101819 -0.109899   \n",
       "9985  0.083080 -0.072640 -0.045678 -0.023598  0.038576  0.138184 -0.061783   \n",
       "9986 -0.014342 -0.117543 -0.016599 -0.007097 -0.061239  0.126527 -0.086190   \n",
       "9987  0.107201 -0.061965 -0.012442 -0.037505  0.031311  0.030624 -0.047779   \n",
       "9988  0.063790 -0.030417  0.006672  0.031840  0.017358  0.092839 -0.073205   \n",
       "9989  0.129673 -0.035070 -0.019253  0.032304  0.048483  0.066526 -0.103398   \n",
       "9990  0.075689 -0.069220  0.005614 -0.009541  0.057809  0.016492 -0.094435   \n",
       "9991  0.060796 -0.036552  0.067404  0.028957  0.029521  0.032277 -0.033691   \n",
       "9992  0.031545 -0.006376  0.035787  0.008093  0.055254  0.090715 -0.078966   \n",
       "9993  0.034930 -0.063043 -0.034943 -0.015523  0.007676  0.115061 -0.098961   \n",
       "9994  0.034875 -0.057653  0.022750 -0.005472  0.022836  0.094878 -0.071061   \n",
       "9995  0.023687 -0.013359  0.030009  0.003973  0.019375  0.070721 -0.090010   \n",
       "9996  0.077506 -0.134495 -0.028633 -0.018219 -0.072727  0.098192 -0.187106   \n",
       "9997  0.073166 -0.029452 -0.003350 -0.016969  0.058090  0.107816 -0.062161   \n",
       "9998  0.110653 -0.019663  0.055734  0.038825  0.017562  0.063298 -0.047049   \n",
       "9999  0.053051 -0.096866  0.059795 -0.005018  0.046272  0.098112 -0.031299   \n",
       "\n",
       "           7         8         9      ...          690       691       692  \\\n",
       "0     0.037434 -0.061646 -0.003507    ...     0.029842  0.034282 -0.007757   \n",
       "1    -0.002381 -0.041077 -0.057780    ...     0.011981  0.018971 -0.004546   \n",
       "2     0.031735 -0.137925 -0.039277    ...     0.010198  0.011956 -0.021904   \n",
       "3     0.012990 -0.082796  0.005155    ...     0.088216 -0.011502 -0.079337   \n",
       "4    -0.018885 -0.050624 -0.046092    ...     0.010539  0.062242 -0.013465   \n",
       "5     0.007401 -0.016551 -0.029035    ...     0.035812  0.033511 -0.007084   \n",
       "6     0.010875 -0.090924 -0.006585    ...     0.109167  0.017601 -0.046829   \n",
       "7    -0.010792 -0.053309 -0.015862    ...     0.036565 -0.030476 -0.027754   \n",
       "8     0.002891 -0.032137  0.001775    ...    -0.038950  0.069045 -0.016575   \n",
       "9    -0.070167 -0.066852  0.062904    ...     0.014400 -0.054573 -0.067236   \n",
       "10   -0.071566 -0.153524  0.016528    ...     0.048192  0.074088  0.033511   \n",
       "11    0.037358 -0.031785 -0.019825    ...     0.071896  0.039093  0.020866   \n",
       "12   -0.026187 -0.082375 -0.017893    ...     0.096461  0.040524  0.031188   \n",
       "13   -0.043617 -0.056923 -0.006193    ...     0.027632  0.030111 -0.016738   \n",
       "14   -0.043240 -0.017142 -0.080966    ...     0.121972  0.031985 -0.049662   \n",
       "15   -0.024485 -0.164675  0.083258    ...     0.000330  0.057275  0.015993   \n",
       "16   -0.004193 -0.010217  0.030054    ...     0.016064  0.051290 -0.039258   \n",
       "17    0.067526 -0.058794  0.001848    ...     0.003592  0.091787  0.017431   \n",
       "18   -0.009517 -0.074717 -0.012298    ...     0.000491  0.026736  0.005279   \n",
       "19   -0.059648 -0.027014 -0.003172    ...     0.077960  0.045765  0.004937   \n",
       "20    0.016152 -0.081509 -0.041772    ...     0.061033  0.031002  0.015491   \n",
       "21    0.022791 -0.041287 -0.007472    ...     0.095265  0.025143  0.003929   \n",
       "22   -0.055836 -0.015922 -0.041905    ...     0.047669  0.018574 -0.027233   \n",
       "23   -0.008223 -0.047173 -0.016688    ...     0.064130  0.043720 -0.014082   \n",
       "24    0.043686 -0.138899 -0.091700    ...     0.037100  0.028538  0.017487   \n",
       "25    0.029453 -0.068578 -0.069237    ...     0.027413  0.011770 -0.027354   \n",
       "26   -0.017488 -0.022667 -0.031037    ...    -0.028382  0.085795 -0.011994   \n",
       "27   -0.055611 -0.083917  0.001391    ...    -0.018890  0.025962 -0.008534   \n",
       "28    0.006816 -0.051740 -0.001837    ...     0.034779 -0.003818  0.054163   \n",
       "29    0.012504 -0.076657  0.051382    ...     0.048832  0.001921 -0.031135   \n",
       "...        ...       ...       ...    ...          ...       ...       ...   \n",
       "9970  0.027010 -0.105847  0.024720    ...     0.016851  0.004394 -0.000292   \n",
       "9971 -0.027949 -0.033807  0.100326    ...     0.057753  0.086985 -0.000887   \n",
       "9972 -0.075296 -0.059033 -0.033685    ...     0.069215 -0.053203  0.012437   \n",
       "9973 -0.030462 -0.110869  0.011546    ...     0.034062 -0.005004 -0.080140   \n",
       "9974  0.035967 -0.026795 -0.002816    ...     0.030413  0.050895 -0.003568   \n",
       "9975  0.016928 -0.036449 -0.035742    ...     0.026605  0.026551 -0.019214   \n",
       "9976  0.007533 -0.073528  0.046852    ...     0.048373  0.022968 -0.038970   \n",
       "9977 -0.007846 -0.083524 -0.026597    ...    -0.025975 -0.086432 -0.056930   \n",
       "9978 -0.006987 -0.036043 -0.014955    ...     0.032801  0.028598  0.055053   \n",
       "9979 -0.037473 -0.060101  0.002025    ...     0.027371  0.051686  0.012803   \n",
       "9980  0.007961  0.004577  0.014514    ...     0.041324  0.057160  0.010906   \n",
       "9981 -0.027256 -0.079023  0.023003    ...    -0.000080  0.092852  0.020809   \n",
       "9982  0.009791 -0.082260 -0.017912    ...    -0.005001  0.017491  0.021114   \n",
       "9983  0.016494 -0.060275 -0.041975    ...     0.013267  0.097718  0.004806   \n",
       "9984  0.014323 -0.055131  0.004745    ...     0.035951  0.066272 -0.044217   \n",
       "9985 -0.057908 -0.072600  0.029508    ...    -0.001042 -0.014972 -0.026103   \n",
       "9986  0.053103 -0.074879  0.017508    ...     0.003397  0.055675 -0.037259   \n",
       "9987 -0.028625 -0.059669  0.027321    ...     0.058339  0.031198  0.021370   \n",
       "9988  0.034291 -0.022704 -0.026061    ...     0.030154  0.022537 -0.000946   \n",
       "9989  0.005715 -0.118712  0.022138    ...    -0.011730  0.102465 -0.051305   \n",
       "9990 -0.021130 -0.098352 -0.011267    ...     0.090040  0.027887 -0.033888   \n",
       "9991 -0.012002 -0.051258 -0.006559    ...     0.047390  0.025762  0.004190   \n",
       "9992 -0.005027 -0.056674 -0.038876    ...     0.017271  0.047317  0.040798   \n",
       "9993  0.008999 -0.022952 -0.035981    ...     0.102981 -0.030225 -0.014419   \n",
       "9994 -0.008741 -0.046399 -0.008402    ...     0.024601  0.050609  0.010503   \n",
       "9995 -0.003628 -0.050935 -0.004465    ...     0.031084  0.043216 -0.004038   \n",
       "9996  0.001064 -0.056606  0.091683    ...     0.100479 -0.015557  0.015917   \n",
       "9997  0.016016 -0.013372 -0.018690    ...     0.014468  0.015748 -0.007151   \n",
       "9998  0.002579 -0.088703 -0.008809    ...     0.011645  0.025317 -0.028721   \n",
       "9999 -0.015067 -0.084755  0.031297    ...    -0.028131  0.062933 -0.016335   \n",
       "\n",
       "           693       694       695       696       697       698       699  \n",
       "0     0.030578 -0.046678  0.035420 -0.004989 -0.010232  0.047805 -0.037694  \n",
       "1    -0.016217 -0.026465  0.018937  0.034689  0.003283  0.011223 -0.032094  \n",
       "2     0.014571 -0.027251  0.023213  0.028321  0.043685 -0.010480 -0.013711  \n",
       "3     0.027293 -0.071953 -0.053504 -0.034882  0.111229  0.024426 -0.039842  \n",
       "4     0.043487 -0.030116  0.037946 -0.004833 -0.031555 -0.021488 -0.015373  \n",
       "5    -0.038157 -0.044625 -0.009350  0.006107  0.029026  0.036696 -0.051071  \n",
       "6     0.026287 -0.031210  0.008301  0.007965  0.068244  0.058199  0.045689  \n",
       "7    -0.033668 -0.089159 -0.007970 -0.080292  0.054372  0.054673 -0.017321  \n",
       "8     0.009035 -0.032397  0.004815 -0.024131  0.004321  0.022271 -0.049432  \n",
       "9    -0.089781  0.017827 -0.053541  0.044077  0.141332 -0.035521 -0.005752  \n",
       "10   -0.005411  0.017689  0.040210 -0.001459  0.063801  0.066629 -0.003736  \n",
       "11    0.038385 -0.044749  0.045367  0.005251 -0.015119  0.065568 -0.020077  \n",
       "12    0.009857 -0.048225  0.049586  0.019818  0.039278  0.004457 -0.050517  \n",
       "13   -0.015255 -0.044684 -0.031177  0.001518 -0.002033  0.049643 -0.047786  \n",
       "14    0.092810 -0.021684  0.037253  0.049969 -0.018936  0.015811  0.004661  \n",
       "15   -0.013844 -0.011650  0.004270 -0.047628  0.075499  0.073852 -0.004789  \n",
       "16    0.006992 -0.074214 -0.005658 -0.027389 -0.030596  0.061851 -0.042742  \n",
       "17    0.015878 -0.062211  0.050115 -0.005342 -0.015358  0.179090 -0.056513  \n",
       "18   -0.046205  0.081218  0.020710 -0.024002  0.007634  0.032666 -0.044400  \n",
       "19   -0.046760 -0.033504  0.015540  0.033273  0.037061 -0.005167  0.007517  \n",
       "20    0.028840 -0.039521  0.006944 -0.007877 -0.048568  0.024348 -0.025827  \n",
       "21   -0.018864 -0.065227 -0.015542  0.026593  0.029376  0.018339 -0.074801  \n",
       "22   -0.005485 -0.040892  0.021075  0.050376 -0.033756  0.047922 -0.042465  \n",
       "23    0.002551 -0.054833  0.027680 -0.037300  0.005150  0.007084 -0.065234  \n",
       "24   -0.038585 -0.067838  0.042451 -0.006711 -0.027048  0.041099 -0.006399  \n",
       "25   -0.013086 -0.036388  0.017089  0.017937  0.006725  0.076809 -0.026545  \n",
       "26    0.026759 -0.066163  0.060391  0.012451 -0.028126 -0.009446 -0.015308  \n",
       "27   -0.072622  0.069724 -0.049076  0.006183  0.041978 -0.021082 -0.028592  \n",
       "28    0.032721 -0.014674  0.037874 -0.007316 -0.079316 -0.002309 -0.079570  \n",
       "29   -0.050066 -0.009962  0.040372  0.128748 -0.022456 -0.011739 -0.041284  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9970  0.016578  0.005779  0.014184 -0.002667  0.080414  0.002684 -0.039569  \n",
       "9971 -0.002473 -0.007079  0.065868  0.022413  0.056527  0.067364 -0.030107  \n",
       "9972 -0.039112 -0.005080 -0.021198  0.007701  0.138480 -0.000740 -0.036735  \n",
       "9973 -0.108227  0.040641 -0.029733  0.019564  0.070517  0.016961 -0.001265  \n",
       "9974  0.026351 -0.056430  0.035133  0.000348 -0.023524  0.056665 -0.040537  \n",
       "9975  0.039910 -0.049488  0.031920  0.001951 -0.011355  0.009898 -0.019989  \n",
       "9976 -0.044702  0.005108  0.032874  0.020721 -0.001512  0.041173  0.003127  \n",
       "9977 -0.048261 -0.088495 -0.045814  0.049602  0.131018 -0.026412  0.004700  \n",
       "9978 -0.098109 -0.017104  0.003911  0.049645  0.007885  0.079296  0.030529  \n",
       "9979 -0.058289 -0.011217  0.021801 -0.018431  0.047807  0.046113 -0.024265  \n",
       "9980  0.088287 -0.037321  0.057943 -0.007911 -0.024081  0.023898 -0.034964  \n",
       "9981 -0.007451 -0.009706  0.013813 -0.042984  0.010979  0.072838 -0.011917  \n",
       "9982 -0.026013  0.020584  0.035028 -0.004941  0.051360  0.038294 -0.017561  \n",
       "9983 -0.035394 -0.035664  0.053732 -0.019229 -0.022298  0.052383 -0.030501  \n",
       "9984  0.021891 -0.089789  0.061103  0.015041 -0.024414  0.096560 -0.035556  \n",
       "9985 -0.038321 -0.056110  0.007828  0.029108  0.028697  0.086552 -0.004248  \n",
       "9986  0.033994 -0.115524 -0.013510  0.070310  0.070588  0.037016 -0.067954  \n",
       "9987 -0.078451  0.060494  0.037231  0.005842 -0.011147  0.031787 -0.027944  \n",
       "9988  0.023961 -0.070943  0.022972  0.012036 -0.048032  0.096727 -0.078986  \n",
       "9989 -0.008031 -0.068241  0.154680  0.066076 -0.045540  0.084392 -0.029411  \n",
       "9990 -0.008269 -0.124192  0.036016 -0.062252  0.045104  0.046578 -0.020783  \n",
       "9991 -0.004225  0.016428  0.067068 -0.021656  0.012830  0.004281 -0.027166  \n",
       "9992  0.000954 -0.075576  0.010327  0.010834 -0.016392  0.074396 -0.036193  \n",
       "9993  0.032484 -0.064793 -0.037512  0.071747  0.048783  0.063552 -0.025651  \n",
       "9994  0.005921 -0.061036  0.023769 -0.023205  0.013706  0.061262 -0.043021  \n",
       "9995  0.016401 -0.047682  0.052034 -0.000792 -0.022946  0.075094 -0.022445  \n",
       "9996  0.010442 -0.012845  0.009642 -0.078654  0.068639  0.158897 -0.055804  \n",
       "9997  0.011598 -0.019393 -0.001319  0.016347  0.029298  0.022122 -0.013759  \n",
       "9998 -0.052768  0.020667  0.002430  0.037796  0.009136  0.031047 -0.042717  \n",
       "9999  0.001387  0.013085 -0.061711  0.034415  0.037902  0.031869 -0.023133  \n",
       "\n",
       "[9999 rows x 700 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>692</th>\n",
       "      <th>693</th>\n",
       "      <th>694</th>\n",
       "      <th>695</th>\n",
       "      <th>696</th>\n",
       "      <th>697</th>\n",
       "      <th>698</th>\n",
       "      <th>699</th>\n",
       "      <th>Department</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007224</td>\n",
       "      <td>-0.018928</td>\n",
       "      <td>-0.015692</td>\n",
       "      <td>0.004884</td>\n",
       "      <td>0.034927</td>\n",
       "      <td>0.064126</td>\n",
       "      <td>-0.054061</td>\n",
       "      <td>0.037434</td>\n",
       "      <td>-0.061646</td>\n",
       "      <td>-0.003507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007757</td>\n",
       "      <td>0.030578</td>\n",
       "      <td>-0.046678</td>\n",
       "      <td>0.035420</td>\n",
       "      <td>-0.004989</td>\n",
       "      <td>-0.010232</td>\n",
       "      <td>0.047805</td>\n",
       "      <td>-0.037694</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.061707</td>\n",
       "      <td>-0.019779</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>-0.002620</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>0.067803</td>\n",
       "      <td>-0.054617</td>\n",
       "      <td>-0.002381</td>\n",
       "      <td>-0.041077</td>\n",
       "      <td>-0.057780</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004546</td>\n",
       "      <td>-0.016217</td>\n",
       "      <td>-0.026465</td>\n",
       "      <td>0.018937</td>\n",
       "      <td>0.034689</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.011223</td>\n",
       "      <td>-0.032094</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062479</td>\n",
       "      <td>-0.016062</td>\n",
       "      <td>0.022250</td>\n",
       "      <td>-0.007306</td>\n",
       "      <td>0.061340</td>\n",
       "      <td>0.070155</td>\n",
       "      <td>-0.063448</td>\n",
       "      <td>0.031735</td>\n",
       "      <td>-0.137925</td>\n",
       "      <td>-0.039277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021904</td>\n",
       "      <td>0.014571</td>\n",
       "      <td>-0.027251</td>\n",
       "      <td>0.023213</td>\n",
       "      <td>0.028321</td>\n",
       "      <td>0.043685</td>\n",
       "      <td>-0.010480</td>\n",
       "      <td>-0.013711</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136225</td>\n",
       "      <td>-0.093007</td>\n",
       "      <td>0.003619</td>\n",
       "      <td>0.026869</td>\n",
       "      <td>0.057067</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>-0.123075</td>\n",
       "      <td>0.012990</td>\n",
       "      <td>-0.082796</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079337</td>\n",
       "      <td>0.027293</td>\n",
       "      <td>-0.071953</td>\n",
       "      <td>-0.053504</td>\n",
       "      <td>-0.034882</td>\n",
       "      <td>0.111229</td>\n",
       "      <td>0.024426</td>\n",
       "      <td>-0.039842</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037710</td>\n",
       "      <td>-0.054166</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>-0.036107</td>\n",
       "      <td>-0.003270</td>\n",
       "      <td>0.021288</td>\n",
       "      <td>-0.063551</td>\n",
       "      <td>-0.018885</td>\n",
       "      <td>-0.050624</td>\n",
       "      <td>-0.046092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013465</td>\n",
       "      <td>0.043487</td>\n",
       "      <td>-0.030116</td>\n",
       "      <td>0.037946</td>\n",
       "      <td>-0.004833</td>\n",
       "      <td>-0.031555</td>\n",
       "      <td>-0.021488</td>\n",
       "      <td>-0.015373</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 702 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.007224 -0.018928 -0.015692  0.004884  0.034927  0.064126 -0.054061   \n",
       "1  0.061707 -0.019779  0.023700 -0.002620  0.036888  0.067803 -0.054617   \n",
       "2  0.062479 -0.016062  0.022250 -0.007306  0.061340  0.070155 -0.063448   \n",
       "3  0.136225 -0.093007  0.003619  0.026869  0.057067  0.102000 -0.123075   \n",
       "4  0.037710 -0.054166  0.011200 -0.036107 -0.003270  0.021288 -0.063551   \n",
       "\n",
       "          7         8         9  ...        692       693       694       695  \\\n",
       "0  0.037434 -0.061646 -0.003507  ...  -0.007757  0.030578 -0.046678  0.035420   \n",
       "1 -0.002381 -0.041077 -0.057780  ...  -0.004546 -0.016217 -0.026465  0.018937   \n",
       "2  0.031735 -0.137925 -0.039277  ...  -0.021904  0.014571 -0.027251  0.023213   \n",
       "3  0.012990 -0.082796  0.005155  ...  -0.079337  0.027293 -0.071953 -0.053504   \n",
       "4 -0.018885 -0.050624 -0.046092  ...  -0.013465  0.043487 -0.030116  0.037946   \n",
       "\n",
       "        696       697       698       699  Department  pred  \n",
       "0 -0.004989 -0.010232  0.047805 -0.037694        57.0   1.0  \n",
       "1  0.034689  0.003283  0.011223 -0.032094        95.0   1.0  \n",
       "2  0.028321  0.043685 -0.010480 -0.013711        84.0   1.0  \n",
       "3 -0.034882  0.111229  0.024426 -0.039842        33.0   0.0  \n",
       "4 -0.004833 -0.031555 -0.021488 -0.015373         8.0   1.0  \n",
       "\n",
       "[5 rows x 702 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dpt = np.unique(x['Department'], return_counts=True)[0]\n",
    "mapping = {'pred':{}}\n",
    "\n",
    "for eval_type in mapping.keys():\n",
    "    for dpt in unique_dpt:\n",
    "        values = x[x['Department']==dpt][eval_type].dropna().values\n",
    "        try:\n",
    "            percentile = np.percentile(abs(values),95)\n",
    "            valid_values = values[abs(values)<=percentile]\n",
    "\n",
    "            M = np.mean(valid_values)\n",
    "            V = np.var(valid_values)\n",
    "        except:\n",
    "            M, V = np.nan, np.nan\n",
    "\n",
    "        mapping[eval_type][dpt] = [M,V]\n",
    "    \n",
    "    \n",
    "    # May need to add this to the dataframe... Then, uncomment the code below\n",
    "    #try:\n",
    "    #    final_data = final_data.drop(eval_type + '_mean_wrt_dpt', axis=1)\n",
    "    #except:\n",
    "    #    pass\n",
    "    #means = []\n",
    "    #for index in final_data.index:\n",
    "    #    means.append(mapping[eval_type][final_data['Department'][index]][0])\n",
    "\n",
    "    #means = pd.DataFrame(means, index=final_data.index, columns=[eval_type + '_mean_wrt_dpt'])\n",
    "    #final_data = pd.concat([final_data, means], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def plot_cartography(Colors, eval_type, figsize=(10, 6)):\n",
    "    # set the filepath and load in a shapefile\n",
    "    fp = '../data/departements-20180101-shp/departements-20180101.shp'\n",
    "    map_df = gpd.read_file(fp)\n",
    "    # check data type so we can see that this is not a normal dataframe, but a GEOdataframe\n",
    "    merged = map_df.set_index('code_insee').join(Colors.set_index('code_insee'))\n",
    "    merged['colors']['2A'] = merged['colors']['2B'] = Colors['colors'][20]\n",
    "    merged['colors']['69D'] = merged['colors']['69M'] = Colors['colors'][69]\n",
    "    for idx in merged.index:\n",
    "        if idx not in Colors['code_insee'].values and idx!='2A' and idx!='2B' and idx!='69D' and idx!='69M':\n",
    "            merged = merged.drop(index=idx)\n",
    "    merged.head()\n",
    "\n",
    "    # set a variable that will call whatever column we want to visualise on the map\n",
    "    variable = 'colors'\n",
    "    # set the range for the choropleth\n",
    "    vmin, vmax = np.min(merged[variable].values)-1e-5, np.max(merged[variable].values)\n",
    "    # create figure and axes for Matplotlib\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "\n",
    "    # create map\n",
    "    merged.plot(column=variable, cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8')\n",
    "\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax.set_title(\"French Departments, colors correpond to the mean of the argumentation scores : \" + eval_type,\n",
    "                 fontdict={'fontsize': '10', 'fontweight' : '3'})\n",
    "\n",
    "    # Create colorbar as a legend\n",
    "    sm = plt.cm.ScalarMappable(cmap='Blues', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "    # empty array for the data range\n",
    "    sm._A = []\n",
    "    # add the colorbar to the figure\n",
    "    cbar = fig.colorbar(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "../data/departements-20180101-shp/departements-20180101.shp: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: ../data/departements-20180101-shp/departements-20180101.shp: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-82cb782572d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                              index=dpts, columns=['code_insee'])], axis=1)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mplot_cartography\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mColors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../{}.png'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-e0f1cbe8482b>\u001b[0m in \u001b[0;36mplot_cartography\u001b[0;34m(Colors, eval_type, figsize)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# set the filepath and load in a shapefile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/departements-20180101-shp/departements-20180101.shp'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmap_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# check data type so we can see that this is not a normal dataframe, but a GEOdataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'code_insee'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mColors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'code_insee'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, bbox, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0;32m--> 253\u001b[0;31m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: ../data/departements-20180101-shp/departements-20180101.shp: No such file or directory"
     ]
    }
   ],
   "source": [
    "dpts = [k for k in range(1,96)]\n",
    "for eval_type in ['pred']:\n",
    "    Colors = []\n",
    "    for dpt in dpts:\n",
    "        Colors.append(mapping[eval_type][dpt][0]) # Because in the geopandas the departments go from 0 to 94\n",
    "\n",
    "    Colors = pd.DataFrame(Colors, index=dpts, columns=['colors'])\n",
    "    Colors = pd.concat([Colors, pd.DataFrame(['0'+str(k) for k in range(1,10)] + [str(k) for k in range(10,96)],\n",
    "                                             index=dpts, columns=['code_insee'])], axis=1)\n",
    "\n",
    "    plot_cartography(Colors, eval_type, figsize=(10,6))\n",
    "    plt.savefig('../../{}.png'.format(eval_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Scores on train---------------\n",
      "[[1073   88]\n",
      " [ 262  533]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.92      0.86      1161\n",
      "        1.0       0.86      0.67      0.75       795\n",
      "\n",
      "avg / total       0.83      0.82      0.82      1956\n",
      "\n",
      "1956 621.0\n",
      "0.7528248587570622\n",
      "---------------Scores on test---------------\n",
      "[[241 111]\n",
      " [ 56  81]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.81      0.68      0.74       352\n",
      "        1.0       0.42      0.59      0.49       137\n",
      "\n",
      "avg / total       0.70      0.66      0.67       489\n",
      "\n",
      "489 137.0\n",
      "0.49240121580547114\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=500)\n",
    "pca.fit_transform(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test) \n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                          multi_class='ovr', C=100.0,\n",
    "                        class_weight='balanced')\n",
    "\n",
    "clf.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "print(15*'-' +'Scores on train' + 15*'-' )\n",
    "y_pred = clf.predict(X_train_pca)\n",
    "print(confusion_matrix(y_pred,y_train))\n",
    "print(classification_report(y_pred, y_train))\n",
    "print(len(y_train), np.sum(y_train))\n",
    "print(f1_score(y_pred,y_train))\n",
    "\n",
    "print(15*'-' +'Scores on test' + 15*'-' )\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(len(y_test), np.sum(y_test))\n",
    "print(f1_score(y_pred,y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Scores on train---------------\n",
      "[[1058   63]\n",
      " [ 277  558]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.94      0.86      1121\n",
      "        1.0       0.90      0.67      0.77       835\n",
      "\n",
      "avg / total       0.84      0.83      0.82      1956\n",
      "\n",
      "1956 621.0\n",
      "0.7664835164835165\n",
      "---------------Scores on test---------------\n",
      "[[256  96]\n",
      " [ 45  92]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.85      0.73      0.78       352\n",
      "        1.0       0.49      0.67      0.57       137\n",
      "\n",
      "avg / total       0.75      0.71      0.72       489\n",
      "\n",
      "489 137.0\n",
      "0.5661538461538461\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=30)\n",
    "pca.fit_transform(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test) \n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=190, max_depth=6, class_weight='balanced')\n",
    "clf.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "print(15*'-' +'Scores on train' + 15*'-' )\n",
    "y_pred = clf.predict(X_train_pca)\n",
    "print(confusion_matrix(y_pred,y_train))\n",
    "print(classification_report(y_pred, y_train))\n",
    "print(len(y_train), np.sum(y_train))\n",
    "print(f1_score(y_pred,y_train))\n",
    "\n",
    "print(15*'-' +'Scores on test' + 15*'-' )\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(len(y_test), np.sum(y_test))\n",
    "print(f1_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Scores on train---------------\n",
      "[[1282    0]\n",
      " [  53  621]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      1.00      0.98      1282\n",
      "        1.0       1.00      0.92      0.96       674\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1956\n",
      "\n",
      "1956 621.0\n",
      "0.959073359073359\n",
      "---------------Scores on test---------------\n",
      "[[293  59]\n",
      " [ 62  75]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.83      0.83      0.83       352\n",
      "        1.0       0.56      0.55      0.55       137\n",
      "\n",
      "avg / total       0.75      0.75      0.75       489\n",
      "\n",
      "489 137.0\n",
      "0.5535055350553505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(base_estimator=clf)\n",
    "clf.fit(X_train_pca, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "\n",
    "print(15*'-' +'Scores on train' + 15*'-' )\n",
    "y_pred = clf.predict(X_train_pca)\n",
    "print(confusion_matrix(y_pred,y_train))\n",
    "print(classification_report(y_pred, y_train))\n",
    "print(len(y_train), np.sum(y_train))\n",
    "print(f1_score(y_pred,y_train))\n",
    "\n",
    "print(15*'-' +'Scores on test' + 15*'-' )\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(len(y_test), np.sum(y_test))\n",
    "print(f1_score(y_pred,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
